{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMSC478 Machine Learning - Spring 2022\n",
    "\n",
    "## Instructor: Fereydoon Vafaei\n",
    "\n",
    "## <font color=\"blue\">Assignment-4: Multi-Class Classification and Regression with Neural Networks</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benjamin Thai MX08618"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Assignment-4, you're going to perform multi-class classification and regression using Neural Networks in Tensorflow/Keras.\n",
    "\n",
    "Pedagogically, this assignment will help you:\n",
    "- better understand how neural networks are built and applied on ML tasks - specifically multi-class classification and regression.\n",
    "- practice NN implementation using Tensorflow 2 and Keras Sequential API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Course Policy Reminder</b>\n",
    "Debugging the codes and error resolution are always the students' responsbility. This policy will be enforced in email communications and the office hours. Keep in mind that all assignments are individual graded tasks. Any collaboration with other students is strictly prohibited and is considered as cheating. Students should NOT share any answer, solution, or code with other students. Violations of these policies would be penalized according to UMBC academic integrity policy.\n",
    "\n",
    "**You must run ALL cells** and get the correct outputs for all cells and give complete answers to all questions. **Cells/codes with no output get zero!**\n",
    "\n",
    "Follow the instructions for each step very carefully.\n",
    "\n",
    "Wherever needed, you should replace `...` elipsis with your code.\n",
    "\n",
    "`...` may indicate one or more lines of missing codes. Some outputs are provided to you to use as reference and to verify that your output is correct.\n",
    "\n",
    "**Preprocessing Effect on Grade**: Preprocessing steps are so critical in each part and you should pay special attention to make sure that you do all preprocessing steps correctly. That is why the reference outputs have been provided in preprocessing steps. Even though preprocessing steps have no direct positive points themselves, they have negative impact if you make mistakes in preprocessing, and your notebook would become unqualified for grading implementation due to wrong results/outputs, and consequently you may get zero for that part of the assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is importing all necessary python, sklearn and Tensorflow/Keras modules. **You definitely need to add to the imports as you work on the assignment.** When you import a new module, add it here in the same cell. All imports should be in this import cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Import Cell\n",
    "    Import necessary Python/Sklearn modules as well as Tensorflow/Keras '''\n",
    "...\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as seaborn\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There should be no error/output after running this cell if the installation is correct\n",
    "tf.debugging.Assert(tf.__version__ >= '2.0.0', [\"You should install Tensorflow 2.x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-I - Multi-Class Classification Using NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part-I, you're going to use tf/keras to build a NN for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the dataset `Sensorless_drive_diagnosis.txt` from [UCI ML Repository page](https://archive.ics.uci.edu/ml/datasets/Dataset+for+Sensorless+Drive+Diagnosis#). The dataset file is in the `Data Folder`.\n",
    "\n",
    "\"Features are extracted from motor current. The motor has intact and defective components. This results in 11 different classes with different conditions.\" You're going to predict these 11 classes. Therefore, this is a multi-class classification problem.\n",
    "\n",
    "Load the text file using pandas `read_csv` considering that the separator is a space `' '`  and the file has no header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58509, 49)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.014600e-07</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.031718</td>\n",
       "      <td>0.03171</td>\n",
       "      <td>0.031721</td>\n",
       "      <td>-0.032963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63308</td>\n",
       "      <td>2.9646</td>\n",
       "      <td>8.1198</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.913200e-06</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.030804</td>\n",
       "      <td>0.03081</td>\n",
       "      <td>0.030806</td>\n",
       "      <td>-0.033520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.59314</td>\n",
       "      <td>7.6252</td>\n",
       "      <td>6.1690</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.951700e-06</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.032877</td>\n",
       "      <td>0.03288</td>\n",
       "      <td>0.032896</td>\n",
       "      <td>-0.029834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63252</td>\n",
       "      <td>2.7784</td>\n",
       "      <td>5.3017</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>-1.4982</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0 -3.014600e-07  0.000008 -0.000012 -0.000002 -0.000001 -0.000021  0.031718   \n",
       "1  2.913200e-06 -0.000005  0.000003 -0.000006  0.000003 -0.000004  0.030804   \n",
       "2 -2.951700e-06 -0.000003 -0.000016 -0.000001 -0.000002  0.000017  0.032877   \n",
       "\n",
       "        7         8         9   ...       39      40      41      42      43  \\\n",
       "0  0.03171  0.031721 -0.032963  ... -0.63308  2.9646  8.1198 -1.4961 -1.4961   \n",
       "1  0.03081  0.030806 -0.033520  ... -0.59314  7.6252  6.1690 -1.4967 -1.4967   \n",
       "2  0.03288  0.032896 -0.029834  ... -0.63252  2.7784  5.3017 -1.4983 -1.4983   \n",
       "\n",
       "       44      45      46      47  48  \n",
       "0 -1.4961 -1.4996 -1.4996 -1.4996   1  \n",
       "1 -1.4967 -1.5005 -1.5005 -1.5005   1  \n",
       "2 -1.4982 -1.4985 -1.4985 -1.4985   1  \n",
       "\n",
       "[3 rows x 49 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = pd.read_csv('Sensorless_drive_diagnosis.txt', delimiter= ' ', header=None)\n",
    "\n",
    "print(data1.shape)\n",
    "data1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the file has no header, column names are integers from 0 to 48, and the last column is the target class. Name the columns from `col1` to `col49` as specified in the provided output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>col7</th>\n",
       "      <th>col8</th>\n",
       "      <th>col9</th>\n",
       "      <th>col10</th>\n",
       "      <th>...</th>\n",
       "      <th>col40</th>\n",
       "      <th>col41</th>\n",
       "      <th>col42</th>\n",
       "      <th>col43</th>\n",
       "      <th>col44</th>\n",
       "      <th>col45</th>\n",
       "      <th>col46</th>\n",
       "      <th>col47</th>\n",
       "      <th>col48</th>\n",
       "      <th>col49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.014600e-07</td>\n",
       "      <td>8.260300e-06</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-1.438600e-06</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.031718</td>\n",
       "      <td>0.031710</td>\n",
       "      <td>0.031721</td>\n",
       "      <td>-0.032963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63308</td>\n",
       "      <td>2.9646</td>\n",
       "      <td>8.1198</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.913200e-06</td>\n",
       "      <td>-5.247700e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>2.778900e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.030804</td>\n",
       "      <td>0.030810</td>\n",
       "      <td>0.030806</td>\n",
       "      <td>-0.033520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.59314</td>\n",
       "      <td>7.6252</td>\n",
       "      <td>6.1690</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.951700e-06</td>\n",
       "      <td>-3.184000e-06</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-1.575300e-06</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.032877</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.032896</td>\n",
       "      <td>-0.029834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63252</td>\n",
       "      <td>2.7784</td>\n",
       "      <td>5.3017</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>-1.4982</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.322600e-06</td>\n",
       "      <td>8.820100e-06</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-7.282900e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.029410</td>\n",
       "      <td>0.029401</td>\n",
       "      <td>0.029417</td>\n",
       "      <td>-0.030156</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.62289</td>\n",
       "      <td>6.5534</td>\n",
       "      <td>6.2606</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4975</td>\n",
       "      <td>-1.4975</td>\n",
       "      <td>-1.4976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.836600e-08</td>\n",
       "      <td>5.666300e-07</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-7.940600e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.030119</td>\n",
       "      <td>0.030119</td>\n",
       "      <td>0.030145</td>\n",
       "      <td>-0.031393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63010</td>\n",
       "      <td>4.5155</td>\n",
       "      <td>9.5231</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           col1          col2      col3      col4          col5      col6  \\\n",
       "0 -3.014600e-07  8.260300e-06 -0.000012 -0.000002 -1.438600e-06 -0.000021   \n",
       "1  2.913200e-06 -5.247700e-06  0.000003 -0.000006  2.778900e-06 -0.000004   \n",
       "2 -2.951700e-06 -3.184000e-06 -0.000016 -0.000001 -1.575300e-06  0.000017   \n",
       "3 -1.322600e-06  8.820100e-06 -0.000016 -0.000005 -7.282900e-07  0.000004   \n",
       "4 -6.836600e-08  5.666300e-07 -0.000026 -0.000006 -7.940600e-07  0.000013   \n",
       "\n",
       "       col7      col8      col9     col10  ...    col40   col41   col42  \\\n",
       "0  0.031718  0.031710  0.031721 -0.032963  ... -0.63308  2.9646  8.1198   \n",
       "1  0.030804  0.030810  0.030806 -0.033520  ... -0.59314  7.6252  6.1690   \n",
       "2  0.032877  0.032880  0.032896 -0.029834  ... -0.63252  2.7784  5.3017   \n",
       "3  0.029410  0.029401  0.029417 -0.030156  ... -0.62289  6.5534  6.2606   \n",
       "4  0.030119  0.030119  0.030145 -0.031393  ... -0.63010  4.5155  9.5231   \n",
       "\n",
       "    col43   col44   col45   col46   col47   col48  col49  \n",
       "0 -1.4961 -1.4961 -1.4961 -1.4996 -1.4996 -1.4996      1  \n",
       "1 -1.4967 -1.4967 -1.4967 -1.5005 -1.5005 -1.5005      1  \n",
       "2 -1.4983 -1.4983 -1.4982 -1.4985 -1.4985 -1.4985      1  \n",
       "3 -1.4963 -1.4963 -1.4963 -1.4975 -1.4975 -1.4976      1  \n",
       "4 -1.4958 -1.4958 -1.4958 -1.4959 -1.4959 -1.4959      1  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = data1.columns\n",
    "data1.columns = np.add(headers, [1])\n",
    "data1 = data1.add_prefix('col')\n",
    "\n",
    "data1.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Rename the last column, i.e. the target column, from `col49` to `class`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>col7</th>\n",
       "      <th>col8</th>\n",
       "      <th>col9</th>\n",
       "      <th>col10</th>\n",
       "      <th>...</th>\n",
       "      <th>col40</th>\n",
       "      <th>col41</th>\n",
       "      <th>col42</th>\n",
       "      <th>col43</th>\n",
       "      <th>col44</th>\n",
       "      <th>col45</th>\n",
       "      <th>col46</th>\n",
       "      <th>col47</th>\n",
       "      <th>col48</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.014600e-07</td>\n",
       "      <td>8.260300e-06</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-1.438600e-06</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.031718</td>\n",
       "      <td>0.031710</td>\n",
       "      <td>0.031721</td>\n",
       "      <td>-0.032963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63308</td>\n",
       "      <td>2.9646</td>\n",
       "      <td>8.1198</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.913200e-06</td>\n",
       "      <td>-5.247700e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>2.778900e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.030804</td>\n",
       "      <td>0.030810</td>\n",
       "      <td>0.030806</td>\n",
       "      <td>-0.033520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.59314</td>\n",
       "      <td>7.6252</td>\n",
       "      <td>6.1690</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.951700e-06</td>\n",
       "      <td>-3.184000e-06</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-1.575300e-06</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.032877</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.032896</td>\n",
       "      <td>-0.029834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63252</td>\n",
       "      <td>2.7784</td>\n",
       "      <td>5.3017</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>-1.4982</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.322600e-06</td>\n",
       "      <td>8.820100e-06</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-7.282900e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.029410</td>\n",
       "      <td>0.029401</td>\n",
       "      <td>0.029417</td>\n",
       "      <td>-0.030156</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.62289</td>\n",
       "      <td>6.5534</td>\n",
       "      <td>6.2606</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4975</td>\n",
       "      <td>-1.4975</td>\n",
       "      <td>-1.4976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.836600e-08</td>\n",
       "      <td>5.666300e-07</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-7.940600e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.030119</td>\n",
       "      <td>0.030119</td>\n",
       "      <td>0.030145</td>\n",
       "      <td>-0.031393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63010</td>\n",
       "      <td>4.5155</td>\n",
       "      <td>9.5231</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           col1          col2      col3      col4          col5      col6  \\\n",
       "0 -3.014600e-07  8.260300e-06 -0.000012 -0.000002 -1.438600e-06 -0.000021   \n",
       "1  2.913200e-06 -5.247700e-06  0.000003 -0.000006  2.778900e-06 -0.000004   \n",
       "2 -2.951700e-06 -3.184000e-06 -0.000016 -0.000001 -1.575300e-06  0.000017   \n",
       "3 -1.322600e-06  8.820100e-06 -0.000016 -0.000005 -7.282900e-07  0.000004   \n",
       "4 -6.836600e-08  5.666300e-07 -0.000026 -0.000006 -7.940600e-07  0.000013   \n",
       "\n",
       "       col7      col8      col9     col10  ...    col40   col41   col42  \\\n",
       "0  0.031718  0.031710  0.031721 -0.032963  ... -0.63308  2.9646  8.1198   \n",
       "1  0.030804  0.030810  0.030806 -0.033520  ... -0.59314  7.6252  6.1690   \n",
       "2  0.032877  0.032880  0.032896 -0.029834  ... -0.63252  2.7784  5.3017   \n",
       "3  0.029410  0.029401  0.029417 -0.030156  ... -0.62289  6.5534  6.2606   \n",
       "4  0.030119  0.030119  0.030145 -0.031393  ... -0.63010  4.5155  9.5231   \n",
       "\n",
       "    col43   col44   col45   col46   col47   col48  class  \n",
       "0 -1.4961 -1.4961 -1.4961 -1.4996 -1.4996 -1.4996      1  \n",
       "1 -1.4967 -1.4967 -1.4967 -1.5005 -1.5005 -1.5005      1  \n",
       "2 -1.4983 -1.4983 -1.4982 -1.4985 -1.4985 -1.4985      1  \n",
       "3 -1.4963 -1.4963 -1.4963 -1.4975 -1.4975 -1.4976      1  \n",
       "4 -1.4958 -1.4958 -1.4958 -1.4959 -1.4959 -1.4959      1  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 = data1.rename(columns={\"col49\": \"class\"})\n",
    "\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There are 11 different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1['class'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The dataset is balanced with respect to 11 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1     5319\n",
       "2     5319\n",
       "3     5319\n",
       "4     5319\n",
       "5     5319\n",
       "6     5319\n",
       "7     5319\n",
       "8     5319\n",
       "9     5319\n",
       "10    5319\n",
       "11    5319\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.groupby('class').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "col1     0\n",
       "col2     0\n",
       "col3     0\n",
       "col4     0\n",
       "col5     0\n",
       "col6     0\n",
       "col7     0\n",
       "col8     0\n",
       "col9     0\n",
       "col10    0\n",
       "col11    0\n",
       "col12    0\n",
       "col13    0\n",
       "col14    0\n",
       "col15    0\n",
       "col16    0\n",
       "col17    0\n",
       "col18    0\n",
       "col19    0\n",
       "col20    0\n",
       "col21    0\n",
       "col22    0\n",
       "col23    0\n",
       "col24    0\n",
       "col25    0\n",
       "col26    0\n",
       "col27    0\n",
       "col28    0\n",
       "col29    0\n",
       "col30    0\n",
       "col31    0\n",
       "col32    0\n",
       "col33    0\n",
       "col34    0\n",
       "col35    0\n",
       "col36    0\n",
       "col37    0\n",
       "col38    0\n",
       "col39    0\n",
       "col40    0\n",
       "col41    0\n",
       "col42    0\n",
       "col43    0\n",
       "col44    0\n",
       "col45    0\n",
       "col46    0\n",
       "col47    0\n",
       "col48    0\n",
       "class    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NAs, and drop NAs if there is any\n",
    "data1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Separate features from the target column, so `X` should contain all feature columns `col1` to `col48` and of course should NOT include `class`. `y` should contain `class` only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58509, 48)\n",
      "(58509,)\n"
     ]
    }
   ],
   "source": [
    "X = data1.drop(columns=\"class\")\n",
    "y = data1['class']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>col4</th>\n",
       "      <th>col5</th>\n",
       "      <th>col6</th>\n",
       "      <th>col7</th>\n",
       "      <th>col8</th>\n",
       "      <th>col9</th>\n",
       "      <th>col10</th>\n",
       "      <th>...</th>\n",
       "      <th>col39</th>\n",
       "      <th>col40</th>\n",
       "      <th>col41</th>\n",
       "      <th>col42</th>\n",
       "      <th>col43</th>\n",
       "      <th>col44</th>\n",
       "      <th>col45</th>\n",
       "      <th>col46</th>\n",
       "      <th>col47</th>\n",
       "      <th>col48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>58509.000000</td>\n",
       "      <td>5.850900e+04</td>\n",
       "      <td>5.850900e+04</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>5.850900e+04</td>\n",
       "      <td>5.850900e+04</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>1.439648e-06</td>\n",
       "      <td>1.412013e-06</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>1.351239e-06</td>\n",
       "      <td>-2.654483e-07</td>\n",
       "      <td>0.001915</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>-0.011897</td>\n",
       "      <td>...</td>\n",
       "      <td>8.406765</td>\n",
       "      <td>-0.397757</td>\n",
       "      <td>7.293781</td>\n",
       "      <td>8.273772</td>\n",
       "      <td>-1.500887</td>\n",
       "      <td>-1.500912</td>\n",
       "      <td>-1.500805</td>\n",
       "      <td>-1.497771</td>\n",
       "      <td>-1.497794</td>\n",
       "      <td>-1.497686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000072</td>\n",
       "      <td>5.555429e-05</td>\n",
       "      <td>2.353009e-04</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>5.660943e-05</td>\n",
       "      <td>2.261907e-04</td>\n",
       "      <td>0.036468</td>\n",
       "      <td>0.036465</td>\n",
       "      <td>0.036470</td>\n",
       "      <td>0.066482</td>\n",
       "      <td>...</td>\n",
       "      <td>6.897301</td>\n",
       "      <td>25.018728</td>\n",
       "      <td>12.451781</td>\n",
       "      <td>6.565952</td>\n",
       "      <td>0.003657</td>\n",
       "      <td>0.003668</td>\n",
       "      <td>0.003632</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>0.003175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.013721</td>\n",
       "      <td>-5.414400e-03</td>\n",
       "      <td>-1.358000e-02</td>\n",
       "      <td>-0.012787</td>\n",
       "      <td>-8.355900e-03</td>\n",
       "      <td>-9.741300e-03</td>\n",
       "      <td>-0.139890</td>\n",
       "      <td>-0.135940</td>\n",
       "      <td>-0.130860</td>\n",
       "      <td>-0.218640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.522180</td>\n",
       "      <td>-0.902350</td>\n",
       "      <td>-0.596830</td>\n",
       "      <td>0.320660</td>\n",
       "      <td>-1.525500</td>\n",
       "      <td>-1.526200</td>\n",
       "      <td>-1.523700</td>\n",
       "      <td>-1.521400</td>\n",
       "      <td>-1.523200</td>\n",
       "      <td>-1.521300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.000007</td>\n",
       "      <td>-1.444400e-05</td>\n",
       "      <td>-7.239600e-05</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-1.475300e-05</td>\n",
       "      <td>-7.379100e-05</td>\n",
       "      <td>-0.019927</td>\n",
       "      <td>-0.019951</td>\n",
       "      <td>-0.019925</td>\n",
       "      <td>-0.032144</td>\n",
       "      <td>...</td>\n",
       "      <td>4.451300</td>\n",
       "      <td>-0.715470</td>\n",
       "      <td>1.450300</td>\n",
       "      <td>4.436300</td>\n",
       "      <td>-1.503300</td>\n",
       "      <td>-1.503400</td>\n",
       "      <td>-1.503200</td>\n",
       "      <td>-1.499600</td>\n",
       "      <td>-1.499600</td>\n",
       "      <td>-1.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000003</td>\n",
       "      <td>8.804600e-07</td>\n",
       "      <td>5.137700e-07</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>7.540200e-07</td>\n",
       "      <td>-1.659300e-07</td>\n",
       "      <td>0.013226</td>\n",
       "      <td>0.013230</td>\n",
       "      <td>0.013247</td>\n",
       "      <td>-0.015566</td>\n",
       "      <td>...</td>\n",
       "      <td>6.566800</td>\n",
       "      <td>-0.661710</td>\n",
       "      <td>3.301300</td>\n",
       "      <td>6.479100</td>\n",
       "      <td>-1.500300</td>\n",
       "      <td>-1.500300</td>\n",
       "      <td>-1.500300</td>\n",
       "      <td>-1.498100</td>\n",
       "      <td>-1.498100</td>\n",
       "      <td>-1.498000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.877700e-05</td>\n",
       "      <td>7.520000e-05</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.906200e-05</td>\n",
       "      <td>7.138600e-05</td>\n",
       "      <td>0.024770</td>\n",
       "      <td>0.024776</td>\n",
       "      <td>0.024777</td>\n",
       "      <td>0.020614</td>\n",
       "      <td>...</td>\n",
       "      <td>9.952600</td>\n",
       "      <td>-0.573980</td>\n",
       "      <td>8.288500</td>\n",
       "      <td>9.857500</td>\n",
       "      <td>-1.498200</td>\n",
       "      <td>-1.498200</td>\n",
       "      <td>-1.498200</td>\n",
       "      <td>-1.496200</td>\n",
       "      <td>-1.496300</td>\n",
       "      <td>-1.496200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.005784</td>\n",
       "      <td>4.525300e-03</td>\n",
       "      <td>5.237700e-03</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>8.245100e-04</td>\n",
       "      <td>2.753600e-03</td>\n",
       "      <td>0.069125</td>\n",
       "      <td>0.069130</td>\n",
       "      <td>0.069131</td>\n",
       "      <td>0.352580</td>\n",
       "      <td>...</td>\n",
       "      <td>265.330000</td>\n",
       "      <td>3670.800000</td>\n",
       "      <td>889.930000</td>\n",
       "      <td>153.150000</td>\n",
       "      <td>-1.457600</td>\n",
       "      <td>-1.456100</td>\n",
       "      <td>-1.455500</td>\n",
       "      <td>-1.337200</td>\n",
       "      <td>-1.337200</td>\n",
       "      <td>-1.337100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               col1          col2          col3          col4          col5  \\\n",
       "count  58509.000000  5.850900e+04  5.850900e+04  58509.000000  5.850900e+04   \n",
       "mean      -0.000003  1.439648e-06  1.412013e-06     -0.000001  1.351239e-06   \n",
       "std        0.000072  5.555429e-05  2.353009e-04      0.000063  5.660943e-05   \n",
       "min       -0.013721 -5.414400e-03 -1.358000e-02     -0.012787 -8.355900e-03   \n",
       "25%       -0.000007 -1.444400e-05 -7.239600e-05     -0.000005 -1.475300e-05   \n",
       "50%       -0.000003  8.804600e-07  5.137700e-07     -0.000001  7.540200e-07   \n",
       "75%        0.000002  1.877700e-05  7.520000e-05      0.000004  1.906200e-05   \n",
       "max        0.005784  4.525300e-03  5.237700e-03      0.001453  8.245100e-04   \n",
       "\n",
       "               col6          col7          col8          col9         col10  \\\n",
       "count  5.850900e+04  58509.000000  58509.000000  58509.000000  58509.000000   \n",
       "mean  -2.654483e-07      0.001915      0.001913      0.001912     -0.011897   \n",
       "std    2.261907e-04      0.036468      0.036465      0.036470      0.066482   \n",
       "min   -9.741300e-03     -0.139890     -0.135940     -0.130860     -0.218640   \n",
       "25%   -7.379100e-05     -0.019927     -0.019951     -0.019925     -0.032144   \n",
       "50%   -1.659300e-07      0.013226      0.013230      0.013247     -0.015566   \n",
       "75%    7.138600e-05      0.024770      0.024776      0.024777      0.020614   \n",
       "max    2.753600e-03      0.069125      0.069130      0.069131      0.352580   \n",
       "\n",
       "       ...         col39         col40         col41         col42  \\\n",
       "count  ...  58509.000000  58509.000000  58509.000000  58509.000000   \n",
       "mean   ...      8.406765     -0.397757      7.293781      8.273772   \n",
       "std    ...      6.897301     25.018728     12.451781      6.565952   \n",
       "min    ...      0.522180     -0.902350     -0.596830      0.320660   \n",
       "25%    ...      4.451300     -0.715470      1.450300      4.436300   \n",
       "50%    ...      6.566800     -0.661710      3.301300      6.479100   \n",
       "75%    ...      9.952600     -0.573980      8.288500      9.857500   \n",
       "max    ...    265.330000   3670.800000    889.930000    153.150000   \n",
       "\n",
       "              col43         col44         col45         col46         col47  \\\n",
       "count  58509.000000  58509.000000  58509.000000  58509.000000  58509.000000   \n",
       "mean      -1.500887     -1.500912     -1.500805     -1.497771     -1.497794   \n",
       "std        0.003657      0.003668      0.003632      0.003163      0.003163   \n",
       "min       -1.525500     -1.526200     -1.523700     -1.521400     -1.523200   \n",
       "25%       -1.503300     -1.503400     -1.503200     -1.499600     -1.499600   \n",
       "50%       -1.500300     -1.500300     -1.500300     -1.498100     -1.498100   \n",
       "75%       -1.498200     -1.498200     -1.498200     -1.496200     -1.496300   \n",
       "max       -1.457600     -1.456100     -1.455500     -1.337200     -1.337200   \n",
       "\n",
       "              col48  \n",
       "count  58509.000000  \n",
       "mean      -1.497686  \n",
       "std        0.003175  \n",
       "min       -1.521300  \n",
       "25%       -1.499500  \n",
       "50%       -1.498000  \n",
       "75%       -1.496200  \n",
       "max       -1.337100  \n",
       "\n",
       "[8 rows x 48 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistical description of the features\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Statistical description of the features reveals that they have different scales, so you need to normalize `X`.\n",
    "\n",
    "> Use [`MinMaxScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to normalize X to the range (0,1) which is the default range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.703460</td>\n",
       "      <td>0.545556</td>\n",
       "      <td>0.721049</td>\n",
       "      <td>0.897795</td>\n",
       "      <td>0.910031</td>\n",
       "      <td>0.777923</td>\n",
       "      <td>0.821032</td>\n",
       "      <td>0.817526</td>\n",
       "      <td>0.812942</td>\n",
       "      <td>0.325053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020749</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.003999</td>\n",
       "      <td>0.051032</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.429387</td>\n",
       "      <td>0.404692</td>\n",
       "      <td>0.118350</td>\n",
       "      <td>0.126882</td>\n",
       "      <td>0.117807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.703624</td>\n",
       "      <td>0.544197</td>\n",
       "      <td>0.721839</td>\n",
       "      <td>0.897532</td>\n",
       "      <td>0.910491</td>\n",
       "      <td>0.779322</td>\n",
       "      <td>0.816659</td>\n",
       "      <td>0.813137</td>\n",
       "      <td>0.808366</td>\n",
       "      <td>0.324078</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011641</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.009233</td>\n",
       "      <td>0.038267</td>\n",
       "      <td>0.424153</td>\n",
       "      <td>0.420827</td>\n",
       "      <td>0.395894</td>\n",
       "      <td>0.113464</td>\n",
       "      <td>0.122043</td>\n",
       "      <td>0.112921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.703324</td>\n",
       "      <td>0.544404</td>\n",
       "      <td>0.720815</td>\n",
       "      <td>0.897872</td>\n",
       "      <td>0.910017</td>\n",
       "      <td>0.781014</td>\n",
       "      <td>0.826577</td>\n",
       "      <td>0.823231</td>\n",
       "      <td>0.818817</td>\n",
       "      <td>0.330531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019933</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.003790</td>\n",
       "      <td>0.032592</td>\n",
       "      <td>0.400589</td>\n",
       "      <td>0.398003</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.124321</td>\n",
       "      <td>0.132796</td>\n",
       "      <td>0.123779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.703407</td>\n",
       "      <td>0.545612</td>\n",
       "      <td>0.720817</td>\n",
       "      <td>0.897619</td>\n",
       "      <td>0.910109</td>\n",
       "      <td>0.779954</td>\n",
       "      <td>0.809990</td>\n",
       "      <td>0.806266</td>\n",
       "      <td>0.801421</td>\n",
       "      <td>0.329967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086379</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.008029</td>\n",
       "      <td>0.038866</td>\n",
       "      <td>0.430044</td>\n",
       "      <td>0.426534</td>\n",
       "      <td>0.401760</td>\n",
       "      <td>0.129750</td>\n",
       "      <td>0.138172</td>\n",
       "      <td>0.128664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.703472</td>\n",
       "      <td>0.544782</td>\n",
       "      <td>0.720284</td>\n",
       "      <td>0.897501</td>\n",
       "      <td>0.910102</td>\n",
       "      <td>0.780702</td>\n",
       "      <td>0.813382</td>\n",
       "      <td>0.809767</td>\n",
       "      <td>0.805061</td>\n",
       "      <td>0.327802</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017129</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>0.005741</td>\n",
       "      <td>0.060214</td>\n",
       "      <td>0.437408</td>\n",
       "      <td>0.433666</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.138436</td>\n",
       "      <td>0.146774</td>\n",
       "      <td>0.137894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.703460  0.545556  0.721049  0.897795  0.910031  0.777923  0.821032   \n",
       "1  0.703624  0.544197  0.721839  0.897532  0.910491  0.779322  0.816659   \n",
       "2  0.703324  0.544404  0.720815  0.897872  0.910017  0.781014  0.826577   \n",
       "3  0.703407  0.545612  0.720817  0.897619  0.910109  0.779954  0.809990   \n",
       "4  0.703472  0.544782  0.720284  0.897501  0.910102  0.780702  0.813382   \n",
       "\n",
       "         7         8         9   ...        38        39        40        41  \\\n",
       "0  0.817526  0.812942  0.325053  ...  0.020749  0.000073  0.003999  0.051032   \n",
       "1  0.813137  0.808366  0.324078  ...  0.011641  0.000084  0.009233  0.038267   \n",
       "2  0.823231  0.818817  0.330531  ...  0.019933  0.000073  0.003790  0.032592   \n",
       "3  0.806266  0.801421  0.329967  ...  0.086379  0.000076  0.008029  0.038866   \n",
       "4  0.809767  0.805061  0.327802  ...  0.017129  0.000074  0.005741  0.060214   \n",
       "\n",
       "         42        43        44        45        46        47  \n",
       "0  0.432990  0.429387  0.404692  0.118350  0.126882  0.117807  \n",
       "1  0.424153  0.420827  0.395894  0.113464  0.122043  0.112921  \n",
       "2  0.400589  0.398003  0.373900  0.124321  0.132796  0.123779  \n",
       "3  0.430044  0.426534  0.401760  0.129750  0.138172  0.128664  \n",
       "4  0.437408  0.433666  0.409091  0.138436  0.146774  0.137894  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use MinMaxSclaer() to normalize X features within the range (0,1) with 0 as minimum and 1 as maximum\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X = pd.DataFrame(scaler.transform(X))\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now the normalized `X` statistics look a lot better! No need to rename the columns to `col1` to `col48` again, we only did it for more clarity before splitting `X` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "      <td>58509.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.703304</td>\n",
       "      <td>0.544870</td>\n",
       "      <td>0.721736</td>\n",
       "      <td>0.897865</td>\n",
       "      <td>0.910335</td>\n",
       "      <td>0.779601</td>\n",
       "      <td>0.678442</td>\n",
       "      <td>0.672225</td>\n",
       "      <td>0.663889</td>\n",
       "      <td>0.361932</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029775</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.008861</td>\n",
       "      <td>0.052039</td>\n",
       "      <td>0.362484</td>\n",
       "      <td>0.360748</td>\n",
       "      <td>0.335711</td>\n",
       "      <td>0.128277</td>\n",
       "      <td>0.136594</td>\n",
       "      <td>0.128200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.003677</td>\n",
       "      <td>0.005589</td>\n",
       "      <td>0.012504</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>0.006166</td>\n",
       "      <td>0.018103</td>\n",
       "      <td>0.174474</td>\n",
       "      <td>0.177820</td>\n",
       "      <td>0.182357</td>\n",
       "      <td>0.116387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026046</td>\n",
       "      <td>0.006814</td>\n",
       "      <td>0.013982</td>\n",
       "      <td>0.042963</td>\n",
       "      <td>0.053864</td>\n",
       "      <td>0.052318</td>\n",
       "      <td>0.053250</td>\n",
       "      <td>0.017171</td>\n",
       "      <td>0.017007</td>\n",
       "      <td>0.017235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.703094</td>\n",
       "      <td>0.543272</td>\n",
       "      <td>0.717814</td>\n",
       "      <td>0.897577</td>\n",
       "      <td>0.908581</td>\n",
       "      <td>0.773716</td>\n",
       "      <td>0.573944</td>\n",
       "      <td>0.565607</td>\n",
       "      <td>0.554700</td>\n",
       "      <td>0.326487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014838</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.026930</td>\n",
       "      <td>0.326951</td>\n",
       "      <td>0.325250</td>\n",
       "      <td>0.300587</td>\n",
       "      <td>0.118350</td>\n",
       "      <td>0.126882</td>\n",
       "      <td>0.118350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.703339</td>\n",
       "      <td>0.544813</td>\n",
       "      <td>0.721688</td>\n",
       "      <td>0.897883</td>\n",
       "      <td>0.910270</td>\n",
       "      <td>0.779609</td>\n",
       "      <td>0.732560</td>\n",
       "      <td>0.727410</td>\n",
       "      <td>0.720567</td>\n",
       "      <td>0.355509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022826</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.004377</td>\n",
       "      <td>0.040296</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.369472</td>\n",
       "      <td>0.343109</td>\n",
       "      <td>0.126493</td>\n",
       "      <td>0.134946</td>\n",
       "      <td>0.126493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.703556</td>\n",
       "      <td>0.546614</td>\n",
       "      <td>0.725657</td>\n",
       "      <td>0.898207</td>\n",
       "      <td>0.912264</td>\n",
       "      <td>0.785335</td>\n",
       "      <td>0.787790</td>\n",
       "      <td>0.783713</td>\n",
       "      <td>0.778220</td>\n",
       "      <td>0.418847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035612</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.009978</td>\n",
       "      <td>0.062402</td>\n",
       "      <td>0.402062</td>\n",
       "      <td>0.399429</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.136808</td>\n",
       "      <td>0.144624</td>\n",
       "      <td>0.136265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0             1             2             3             4   \\\n",
       "count  58509.000000  58509.000000  58509.000000  58509.000000  58509.000000   \n",
       "mean       0.703304      0.544870      0.721736      0.897865      0.910335   \n",
       "std        0.003677      0.005589      0.012504      0.004396      0.006166   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.703094      0.543272      0.717814      0.897577      0.908581   \n",
       "50%        0.703339      0.544813      0.721688      0.897883      0.910270   \n",
       "75%        0.703556      0.546614      0.725657      0.898207      0.912264   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 5             6             7             8             9   \\\n",
       "count  58509.000000  58509.000000  58509.000000  58509.000000  58509.000000   \n",
       "mean       0.779601      0.678442      0.672225      0.663889      0.361932   \n",
       "std        0.018103      0.174474      0.177820      0.182357      0.116387   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.773716      0.573944      0.565607      0.554700      0.326487   \n",
       "50%        0.779609      0.732560      0.727410      0.720567      0.355509   \n",
       "75%        0.785335      0.787790      0.783713      0.778220      0.418847   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       ...            38            39            40            41  \\\n",
       "count  ...  58509.000000  58509.000000  58509.000000  58509.000000   \n",
       "mean   ...      0.029775      0.000137      0.008861      0.052039   \n",
       "std    ...      0.026046      0.006814      0.013982      0.042963   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.014838      0.000051      0.002299      0.026930   \n",
       "50%    ...      0.022826      0.000066      0.004377      0.040296   \n",
       "75%    ...      0.035612      0.000089      0.009978      0.062402   \n",
       "max    ...      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 42            43            44            45            46  \\\n",
       "count  58509.000000  58509.000000  58509.000000  58509.000000  58509.000000   \n",
       "mean       0.362484      0.360748      0.335711      0.128277      0.136594   \n",
       "std        0.053864      0.052318      0.053250      0.017171      0.017007   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.326951      0.325250      0.300587      0.118350      0.126882   \n",
       "50%        0.371134      0.369472      0.343109      0.126493      0.134946   \n",
       "75%        0.402062      0.399429      0.373900      0.136808      0.144624   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "                 47  \n",
       "count  58509.000000  \n",
       "mean       0.128200  \n",
       "std        0.017235  \n",
       "min        0.000000  \n",
       "25%        0.118350  \n",
       "50%        0.126493  \n",
       "75%        0.136265  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 48 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using pandas [`get_dummies()`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) method, convert `y` to one-hot encoded format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58509, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58504</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58505</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58506</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58507</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58508</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>58509 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1   2   3   4   5   6   7   8   9   10  11\n",
       "0       1   0   0   0   0   0   0   0   0   0   0\n",
       "1       1   0   0   0   0   0   0   0   0   0   0\n",
       "2       1   0   0   0   0   0   0   0   0   0   0\n",
       "3       1   0   0   0   0   0   0   0   0   0   0\n",
       "4       1   0   0   0   0   0   0   0   0   0   0\n",
       "...    ..  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
       "58504   0   0   0   0   0   0   0   0   0   0   1\n",
       "58505   0   0   0   0   0   0   0   0   0   0   1\n",
       "58506   0   0   0   0   0   0   0   0   0   0   1\n",
       "58507   0   0   0   0   0   0   0   0   0   0   1\n",
       "58508   0   0   0   0   0   0   0   0   0   0   1\n",
       "\n",
       "[58509 rows x 11 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(y.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You need to convert `X` and `y` to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X = X.values\n",
    "y = y.values\n",
    "print(type(X))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46807, 48)\n",
      "(46807, 11)\n",
      "(11702, 48)\n",
      "(11702, 11)\n"
     ]
    }
   ],
   "source": [
    "# Split X, y to train and test with ratio of 80/20 for train/test respectively\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Baseline NN Model for Multi-Class Classification with 85% Validation Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can begin with a simple neural network `baseline_model` with one or two hidden layers, 5-10 neurons per hidden layer, and increase the number of neurons and hidden layers only if needed. You may also use callback and early stopping to find the optimal number of epochs, but it's possible to obtain **the minimum required `val_accuracy` for the baseline model (0.85)** with 20 epochs and a couple of hidden layers only.\n",
    "\n",
    "> <font color='red'>**val_accuracy Requirement**</font>: The minimum required `val_accuracy` of the `baseline_model` after the last epoch of training is 0.85.\n",
    "\n",
    "> **Hints**:\n",
    "\n",
    "> - Since you're not working with image data in this assignment, you should NOT use `Flatten` layers in any of the NNs of this assignment.\n",
    "\n",
    "> - `input_dim` of the first layer should match with the number of features in `X`.\n",
    "\n",
    "> - You may use ReLU for all hidden layers, but you may also try other activation functions for the hidden layers.\n",
    "\n",
    "> - **The most common mistake** is setting wrong activation function and number of neurons in the output layer; recall that the output layer specifications are determined by the type of ML task i.e. Multi-class Classification.\n",
    "\n",
    "> - The loss function in `compile` should match with the one-hot encoded labels in `y` (check tf-notebook instructions and the links to tf documentations).\n",
    "\n",
    "> - During training, you should see a clear trend of decreasing loss and increasing accuracy in the first few epochs; otherwise, that is a red flag that your model has not been developed properly, thus, stop training, fix the issues and then train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Very Important Note**: Training NNs and DNNs take a lot of time and efforts, and you need to do lots of experiments. While the hints and the provided outputs can be helpful, you are responsible to explore, investigate, and try as many necessary steps and approaches as needed and use the discussions and contents of the lectures/textbook/slides to achieve the desired results and to get credit for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' build the baseline_model\n",
    "    be careful about the input_dim and the output layer specifications\n",
    "    start with a simple model and change the model hyperparameters as needed '''\n",
    "baseline_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(48,  activation='relu', input_shape=[48]), \n",
    "  tf.keras.layers.Dense(11, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, 48)                2352      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 11)                539       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,891\n",
      "Trainable params: 2,891\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "''' get the baseline_model summary '''\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' compile the baseline_model\n",
    "    be careful with the loss function, it should work with one-hot encoded labels\n",
    "    metric should be accuracy\n",
    "    you can choose your optimizer and learning_rate '''\n",
    "\n",
    "baseline_model.compile(loss=\"categorical_crossentropy\", metrics=['accuracy'], optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 1.7574 - accuracy: 0.4490 - val_loss: 1.2736 - val_accuracy: 0.6435\n",
      "Epoch 2/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 1.0438 - accuracy: 0.7019 - val_loss: 0.8816 - val_accuracy: 0.7840\n",
      "Epoch 3/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.7705 - accuracy: 0.7811 - val_loss: 0.7044 - val_accuracy: 0.8240\n",
      "Epoch 4/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.6360 - accuracy: 0.8110 - val_loss: 0.6038 - val_accuracy: 0.8344\n",
      "Epoch 5/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.5603 - accuracy: 0.8254 - val_loss: 0.5416 - val_accuracy: 0.8297\n",
      "Epoch 6/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.5147 - accuracy: 0.8299 - val_loss: 0.5059 - val_accuracy: 0.8432\n",
      "Epoch 7/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.4843 - accuracy: 0.8351 - val_loss: 0.4753 - val_accuracy: 0.8477\n",
      "Epoch 8/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.4630 - accuracy: 0.8376 - val_loss: 0.4574 - val_accuracy: 0.8541\n",
      "Epoch 9/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.4466 - accuracy: 0.8419 - val_loss: 0.4610 - val_accuracy: 0.8344\n",
      "Epoch 10/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.4333 - accuracy: 0.8435 - val_loss: 0.4368 - val_accuracy: 0.8537\n",
      "Epoch 11/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.4218 - accuracy: 0.8454 - val_loss: 0.4330 - val_accuracy: 0.8520\n",
      "Epoch 12/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.4116 - accuracy: 0.8467 - val_loss: 0.4088 - val_accuracy: 0.8592\n",
      "Epoch 13/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.4035 - accuracy: 0.8496 - val_loss: 0.4123 - val_accuracy: 0.8490\n",
      "Epoch 14/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.3956 - accuracy: 0.8521 - val_loss: 0.4017 - val_accuracy: 0.8616\n",
      "Epoch 15/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.3882 - accuracy: 0.8535 - val_loss: 0.4042 - val_accuracy: 0.8560\n",
      "Epoch 16/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.3827 - accuracy: 0.8547 - val_loss: 0.3817 - val_accuracy: 0.8703\n",
      "Epoch 17/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.3774 - accuracy: 0.8573 - val_loss: 0.3809 - val_accuracy: 0.8637\n",
      "Epoch 18/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.3710 - accuracy: 0.8575 - val_loss: 0.3849 - val_accuracy: 0.8635\n",
      "Epoch 19/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.3647 - accuracy: 0.8618 - val_loss: 0.3715 - val_accuracy: 0.8703\n",
      "Epoch 20/20\n",
      "1317/1317 [==============================] - 3s 3ms/step - loss: 0.3604 - accuracy: 0.8623 - val_loss: 0.3712 - val_accuracy: 0.8714\n"
     ]
    }
   ],
   "source": [
    "''' train the baseline_model on X_train, y_train with 20 epochs and a validation_split=0.1\n",
    "    You may increase number of epochs or use callbacks and early stopping if needed\n",
    "    The minimum required val_accuracy after the last epoch is 0.85 '''\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"part1_Baseline_classification_model.h5\", save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n",
    "\n",
    "baseline_model_history = baseline_model.fit(X_train, y_train, epochs=20, validation_split=0.1, callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' plot the history with figsize=(10,5)\n",
    "    the plot should display the grid and the whole range of values for loss and accuracy '''\n",
    "\n",
    "pd.DataFrame(baseline_model_history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1.8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: After each training session, the model contains some trained weights, so if you want to make changes to your NN and re-run your experiments with a new random initilization of weights, you should re-run the build-model cell to clear and re-initialize the weights randomly, and then compile it again so that you can have a fresh restart of your updated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building `nn_clf` with 99% Test Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a working `baseline_model` with 85% `val_accuracy`, you should build a NN classifier `nn_clf` that can achieve a **test accuracy** of 99%. You can start with the same architecture of your `baseline_model` and increase the compleixty of `nn_clf` gradually if needed. Recall that some train/test splits are easier for the model to learn from, so if your accuracy is getting so close but not hitting 0.99, you may want to re-split train/test in addition to the changes you may want to make on your model.\n",
    "\n",
    "> **Note**: Any hint given in this notebook is just a suggestion and may or may not work with your model depending on the configurations of your model, so you should try as many different configurations, techniques, and approaches as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_264 (Dense)           (None, 48)                2352      \n",
      "                                                                 \n",
      " dense_265 (Dense)           (None, 48)                2352      \n",
      "                                                                 \n",
      " dense_266 (Dense)           (None, 11)                539       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,243\n",
      "Trainable params: 5,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build nn_clf\n",
    "nn_clf = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(48, activation='LeakyReLU', input_shape=[48]), \n",
    "    tf.keras.layers.Dense(48, activation='LeakyReLU'),\n",
    "    tf.keras.layers.Dense(11, activation='softmax')])\n",
    "nn_clf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Compile nn_clf - metric is 'accuracy' and be careful to choose the loss properly\n",
    "    Hint1: One of the hyperparameters you can change is the optimizer (Adam, RMSprop, SGD, ...)\n",
    "    Hint2: The other impactful hyperparameter is learning_rate '''\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.1,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96)\n",
    "nn_clf.compile(loss=\"categorical_crossentropy\", metrics=['accuracy'], optimizer=keras.optimizers.Adam(learning_rate =lr_schedule))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "1317/1317 [==============================] - 9s 6ms/step - loss: 18.9816 - accuracy: 0.5390 - val_loss: 49.7846 - val_accuracy: 0.4796\n",
      "Epoch 2/120\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 3.5103 - accuracy: 0.7210 - val_loss: 1.6270 - val_accuracy: 0.6960\n",
      "Epoch 3/120\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 1.2919 - accuracy: 0.7582 - val_loss: 0.9343 - val_accuracy: 0.7601\n",
      "Epoch 4/120\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.9205 - accuracy: 0.7669 - val_loss: 0.8644 - val_accuracy: 0.7460\n",
      "Epoch 5/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 11.0919 - accuracy: 0.7218 - val_loss: 1.0240 - val_accuracy: 0.7599\n",
      "Epoch 6/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.9897 - accuracy: 0.7829 - val_loss: 0.5663 - val_accuracy: 0.8154\n",
      "Epoch 7/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.9809 - accuracy: 0.7836 - val_loss: 0.7613 - val_accuracy: 0.7701\n",
      "Epoch 8/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 1.1888 - accuracy: 0.7642 - val_loss: 1.0493 - val_accuracy: 0.7304\n",
      "Epoch 9/120\n",
      "1317/1317 [==============================] - 3s 3ms/step - loss: 0.6976 - accuracy: 0.7826 - val_loss: 0.4563 - val_accuracy: 0.8022\n",
      "Epoch 10/120\n",
      "1317/1317 [==============================] - 3s 3ms/step - loss: 5.6729 - accuracy: 0.7472 - val_loss: 0.6797 - val_accuracy: 0.8206\n",
      "Epoch 11/120\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.7343 - accuracy: 0.8191 - val_loss: 0.9666 - val_accuracy: 0.7565\n",
      "Epoch 12/120\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.6109 - accuracy: 0.8157 - val_loss: 0.4566 - val_accuracy: 0.8248\n",
      "Epoch 13/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.5055 - accuracy: 0.8272 - val_loss: 0.3349 - val_accuracy: 0.8528\n",
      "Epoch 14/120\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.6155 - accuracy: 0.8048 - val_loss: 0.5864 - val_accuracy: 0.7849\n",
      "Epoch 15/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.5367 - accuracy: 0.8139 - val_loss: 0.5609 - val_accuracy: 0.7793\n",
      "Epoch 16/120\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.8789 - accuracy: 0.8123 - val_loss: 0.3375 - val_accuracy: 0.8567\n",
      "Epoch 17/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.4408 - accuracy: 0.8417 - val_loss: 0.3932 - val_accuracy: 0.8310\n",
      "Epoch 18/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.4509 - accuracy: 0.8343 - val_loss: 0.4619 - val_accuracy: 0.7964\n",
      "Epoch 19/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 1.0538 - accuracy: 0.8123 - val_loss: 0.4459 - val_accuracy: 0.8355\n",
      "Epoch 20/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.4210 - accuracy: 0.8479 - val_loss: 0.3378 - val_accuracy: 0.8594\n",
      "Epoch 21/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.4188 - accuracy: 0.8479 - val_loss: 0.2897 - val_accuracy: 0.8746\n",
      "Epoch 22/120\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.3966 - accuracy: 0.8543 - val_loss: 0.3835 - val_accuracy: 0.8353\n",
      "Epoch 23/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.3510 - accuracy: 0.8663 - val_loss: 0.2556 - val_accuracy: 0.8987\n",
      "Epoch 24/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.3644 - accuracy: 0.8646 - val_loss: 0.5065 - val_accuracy: 0.7953\n",
      "Epoch 25/120\n",
      "1317/1317 [==============================] - 3s 3ms/step - loss: 0.3518 - accuracy: 0.8685 - val_loss: 0.2932 - val_accuracy: 0.8746\n",
      "Epoch 26/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.3051 - accuracy: 0.8838 - val_loss: 0.2496 - val_accuracy: 0.8972\n",
      "Epoch 27/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.3262 - accuracy: 0.8779 - val_loss: 0.2438 - val_accuracy: 0.9071\n",
      "Epoch 28/120\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.2933 - accuracy: 0.8901 - val_loss: 0.2762 - val_accuracy: 0.8838\n",
      "Epoch 29/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2764 - accuracy: 0.8941 - val_loss: 0.2810 - val_accuracy: 0.8906\n",
      "Epoch 30/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.3070 - accuracy: 0.8892 - val_loss: 0.3637 - val_accuracy: 0.8725\n",
      "Epoch 31/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2632 - accuracy: 0.9005 - val_loss: 0.1858 - val_accuracy: 0.9299\n",
      "Epoch 32/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2590 - accuracy: 0.9056 - val_loss: 0.2037 - val_accuracy: 0.9225\n",
      "Epoch 33/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2383 - accuracy: 0.9117 - val_loss: 0.1869 - val_accuracy: 0.9301\n",
      "Epoch 34/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2487 - accuracy: 0.9074 - val_loss: 0.2380 - val_accuracy: 0.9045\n",
      "Epoch 35/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2396 - accuracy: 0.9126 - val_loss: 0.1874 - val_accuracy: 0.9272\n",
      "Epoch 36/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2224 - accuracy: 0.9173 - val_loss: 0.1990 - val_accuracy: 0.9197\n",
      "Epoch 37/120\n",
      "1317/1317 [==============================] - 3s 3ms/step - loss: 0.2228 - accuracy: 0.9187 - val_loss: 0.2036 - val_accuracy: 0.9160\n",
      "Epoch 38/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2179 - accuracy: 0.9186 - val_loss: 0.1714 - val_accuracy: 0.9321\n",
      "Epoch 39/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2085 - accuracy: 0.9233 - val_loss: 0.1813 - val_accuracy: 0.9344\n",
      "Epoch 40/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1964 - accuracy: 0.9285 - val_loss: 0.1552 - val_accuracy: 0.9402\n",
      "Epoch 41/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1969 - accuracy: 0.9295 - val_loss: 0.2048 - val_accuracy: 0.9182\n",
      "Epoch 42/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1906 - accuracy: 0.9315 - val_loss: 0.1850 - val_accuracy: 0.9263\n",
      "Epoch 43/120\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1801 - accuracy: 0.9331 - val_loss: 0.1360 - val_accuracy: 0.9464\n",
      "Epoch 44/120\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.1694 - accuracy: 0.9393 - val_loss: 0.1173 - val_accuracy: 0.9613\n",
      "Epoch 45/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1652 - accuracy: 0.9405 - val_loss: 0.1373 - val_accuracy: 0.9496\n",
      "Epoch 46/120\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1611 - accuracy: 0.9438 - val_loss: 0.1115 - val_accuracy: 0.9603\n",
      "Epoch 47/120\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.1532 - accuracy: 0.9458 - val_loss: 0.1694 - val_accuracy: 0.9319\n",
      "Epoch 48/120\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1438 - accuracy: 0.9489 - val_loss: 0.1340 - val_accuracy: 0.9517\n",
      "Epoch 49/120\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.1400 - accuracy: 0.9503 - val_loss: 0.1394 - val_accuracy: 0.9532\n",
      "Epoch 50/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1338 - accuracy: 0.9531 - val_loss: 0.0935 - val_accuracy: 0.9709\n",
      "Epoch 51/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1291 - accuracy: 0.9539 - val_loss: 0.1288 - val_accuracy: 0.9566\n",
      "Epoch 52/120\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1306 - accuracy: 0.9549 - val_loss: 0.1005 - val_accuracy: 0.9590\n",
      "Epoch 53/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1218 - accuracy: 0.9575 - val_loss: 0.1082 - val_accuracy: 0.9626\n",
      "Epoch 54/120\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.1103 - accuracy: 0.9622 - val_loss: 0.0732 - val_accuracy: 0.9739\n",
      "Epoch 55/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1064 - accuracy: 0.9629 - val_loss: 0.0890 - val_accuracy: 0.9692\n",
      "Epoch 56/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1034 - accuracy: 0.9631 - val_loss: 0.0930 - val_accuracy: 0.9682\n",
      "Epoch 57/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0972 - accuracy: 0.9655 - val_loss: 0.0777 - val_accuracy: 0.9729\n",
      "Epoch 58/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0918 - accuracy: 0.9668 - val_loss: 0.0716 - val_accuracy: 0.9716\n",
      "Epoch 59/120\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.0923 - accuracy: 0.9677 - val_loss: 0.0683 - val_accuracy: 0.9727\n",
      "Epoch 60/120\n",
      "1317/1317 [==============================] - 7s 6ms/step - loss: 0.0855 - accuracy: 0.9696 - val_loss: 0.0673 - val_accuracy: 0.9742\n",
      "Epoch 61/120\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.0811 - accuracy: 0.9707 - val_loss: 0.0635 - val_accuracy: 0.9754\n",
      "Epoch 62/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0777 - accuracy: 0.9718 - val_loss: 0.0615 - val_accuracy: 0.9780\n",
      "Epoch 63/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0732 - accuracy: 0.9737 - val_loss: 0.0647 - val_accuracy: 0.9752\n",
      "Epoch 64/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0723 - accuracy: 0.9740 - val_loss: 0.0657 - val_accuracy: 0.9763\n",
      "Epoch 65/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0691 - accuracy: 0.9759 - val_loss: 0.0586 - val_accuracy: 0.9776\n",
      "Epoch 66/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0649 - accuracy: 0.9766 - val_loss: 0.0509 - val_accuracy: 0.9814\n",
      "Epoch 67/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0619 - accuracy: 0.9778 - val_loss: 0.0761 - val_accuracy: 0.9705\n",
      "Epoch 68/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0616 - accuracy: 0.9784 - val_loss: 0.0717 - val_accuracy: 0.9720\n",
      "Epoch 69/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0626 - accuracy: 0.9784 - val_loss: 0.0413 - val_accuracy: 0.9872\n",
      "Epoch 70/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0553 - accuracy: 0.9806 - val_loss: 0.0498 - val_accuracy: 0.9827\n",
      "Epoch 71/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0528 - accuracy: 0.9817 - val_loss: 0.0473 - val_accuracy: 0.9848\n",
      "Epoch 72/120\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.0525 - accuracy: 0.9815 - val_loss: 0.0471 - val_accuracy: 0.9848\n",
      "Epoch 73/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0489 - accuracy: 0.9830 - val_loss: 0.0446 - val_accuracy: 0.9855\n",
      "Epoch 74/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0482 - accuracy: 0.9830 - val_loss: 0.0524 - val_accuracy: 0.9821\n",
      "Epoch 75/120\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.0474 - accuracy: 0.9835 - val_loss: 0.0424 - val_accuracy: 0.9876\n",
      "Epoch 76/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0453 - accuracy: 0.9842 - val_loss: 0.0486 - val_accuracy: 0.9831\n",
      "Epoch 77/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0439 - accuracy: 0.9843 - val_loss: 0.0369 - val_accuracy: 0.9876\n",
      "Epoch 78/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0419 - accuracy: 0.9854 - val_loss: 0.0390 - val_accuracy: 0.9865\n",
      "Epoch 79/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0418 - accuracy: 0.9854 - val_loss: 0.0381 - val_accuracy: 0.9874\n",
      "Epoch 80/120\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.0386 - accuracy: 0.9865 - val_loss: 0.0370 - val_accuracy: 0.9887\n",
      "Epoch 81/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0385 - accuracy: 0.9864 - val_loss: 0.0402 - val_accuracy: 0.9855\n",
      "Epoch 82/120\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.0359 - accuracy: 0.9879 - val_loss: 0.0366 - val_accuracy: 0.9872\n",
      "Epoch 83/120\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.0362 - accuracy: 0.9873 - val_loss: 0.0322 - val_accuracy: 0.9902\n",
      "Epoch 84/120\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.0356 - accuracy: 0.9879 - val_loss: 0.0313 - val_accuracy: 0.9900\n",
      "Epoch 85/120\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.0337 - accuracy: 0.9880 - val_loss: 0.0423 - val_accuracy: 0.9840\n",
      "Epoch 86/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0329 - accuracy: 0.9888 - val_loss: 0.0344 - val_accuracy: 0.9900\n",
      "Epoch 87/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0324 - accuracy: 0.9885 - val_loss: 0.0298 - val_accuracy: 0.9897\n",
      "Epoch 88/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0313 - accuracy: 0.9894 - val_loss: 0.0283 - val_accuracy: 0.9902\n",
      "Epoch 89/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0299 - accuracy: 0.9900 - val_loss: 0.0268 - val_accuracy: 0.9904\n",
      "Epoch 90/120\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.0300 - accuracy: 0.9898 - val_loss: 0.0290 - val_accuracy: 0.9891\n",
      "Epoch 91/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0290 - accuracy: 0.9903 - val_loss: 0.0330 - val_accuracy: 0.9897\n",
      "Epoch 92/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0283 - accuracy: 0.9909 - val_loss: 0.0313 - val_accuracy: 0.9876\n",
      "Epoch 93/120\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.0279 - accuracy: 0.9904 - val_loss: 0.0290 - val_accuracy: 0.9906\n",
      "Epoch 94/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0269 - accuracy: 0.9910 - val_loss: 0.0292 - val_accuracy: 0.9906\n",
      "Epoch 95/120\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.0264 - accuracy: 0.9913 - val_loss: 0.0270 - val_accuracy: 0.9904\n",
      "Epoch 96/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0258 - accuracy: 0.9915 - val_loss: 0.0301 - val_accuracy: 0.9891\n",
      "Epoch 97/120\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.0254 - accuracy: 0.9913 - val_loss: 0.0276 - val_accuracy: 0.9912\n",
      "Epoch 98/120\n",
      "1317/1317 [==============================] - 3s 3ms/step - loss: 0.0245 - accuracy: 0.9922 - val_loss: 0.0281 - val_accuracy: 0.9904\n",
      "Epoch 99/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0246 - accuracy: 0.9924 - val_loss: 0.0279 - val_accuracy: 0.9906\n",
      "Epoch 100/120\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.0240 - accuracy: 0.9925 - val_loss: 0.0257 - val_accuracy: 0.9917\n",
      "Epoch 101/120\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.0236 - accuracy: 0.9924 - val_loss: 0.0268 - val_accuracy: 0.9915\n",
      "Epoch 102/120\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.0230 - accuracy: 0.9932 - val_loss: 0.0252 - val_accuracy: 0.9917\n",
      "Epoch 103/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0227 - accuracy: 0.9934 - val_loss: 0.0248 - val_accuracy: 0.9915\n",
      "Epoch 104/120\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.0226 - accuracy: 0.9926 - val_loss: 0.0237 - val_accuracy: 0.9927\n",
      "Epoch 105/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0221 - accuracy: 0.9930 - val_loss: 0.0237 - val_accuracy: 0.9917\n",
      "Epoch 106/120\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.0218 - accuracy: 0.9928 - val_loss: 0.0233 - val_accuracy: 0.9912\n",
      "Epoch 107/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0212 - accuracy: 0.9933 - val_loss: 0.0223 - val_accuracy: 0.9927\n",
      "Epoch 108/120\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.0212 - accuracy: 0.9931 - val_loss: 0.0231 - val_accuracy: 0.9921\n",
      "Epoch 109/120\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.0204 - accuracy: 0.9935 - val_loss: 0.0268 - val_accuracy: 0.9912\n",
      "Epoch 110/120\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.0209 - accuracy: 0.9937 - val_loss: 0.0223 - val_accuracy: 0.9927\n",
      "Epoch 111/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0201 - accuracy: 0.9938 - val_loss: 0.0244 - val_accuracy: 0.9925\n",
      "Epoch 112/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0200 - accuracy: 0.9939 - val_loss: 0.0223 - val_accuracy: 0.9923\n",
      "Epoch 113/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0199 - accuracy: 0.9938 - val_loss: 0.0227 - val_accuracy: 0.9927\n",
      "Epoch 114/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0194 - accuracy: 0.9941 - val_loss: 0.0221 - val_accuracy: 0.9932\n",
      "Epoch 115/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0192 - accuracy: 0.9941 - val_loss: 0.0223 - val_accuracy: 0.9925\n",
      "Epoch 116/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0192 - accuracy: 0.9940 - val_loss: 0.0231 - val_accuracy: 0.9921\n",
      "Epoch 117/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0191 - accuracy: 0.9941 - val_loss: 0.0215 - val_accuracy: 0.9938\n",
      "Epoch 118/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0187 - accuracy: 0.9945 - val_loss: 0.0216 - val_accuracy: 0.9927\n",
      "Epoch 119/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0187 - accuracy: 0.9942 - val_loss: 0.0228 - val_accuracy: 0.9925\n",
      "Epoch 120/120\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.0185 - accuracy: 0.9944 - val_loss: 0.0221 - val_accuracy: 0.9934\n"
     ]
    }
   ],
   "source": [
    "''' train nn_clf on X_train, y_train with validation_split=0.1\n",
    "     Hint: You may use EarlyStopping and set the patience parameter,\n",
    "     but then you should check in the following cell whether the \"test\" accuracy reaches to 0.99,\n",
    "     and make changes to compile and nn_clf hyperparameters if needed '''\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"part1_nn_clf_99_model.h5\", save_best_only=True)\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=25, restore_best_weights=True)\n",
    "\n",
    "nn_clf_history = nn_clf.fit(X_train, y_train, epochs=120, validation_split=0.1, callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.4)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' plot the history with a figsize of (10,5)\n",
    "    the plot should display the grid and the whole range of values for loss and accuracy '''\n",
    "pd.DataFrame(nn_clf_history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1.4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To evaluate the model, you should use `evaluate()` method on the test set `X_test`.\n",
    "\n",
    "> <font color='red'>**Test Accuracy Requirement**</font>: Your accuracy on `X_test` should be **0.99** (rounded with two decimal places). Otherwise, your `nn_clf` will get no points for this part, so you should fine-tune the hyperparametrs of your `nn_clf` (number of neurons, hidden layers, etc.) and `compile` (such as optimizer, learning rate, etc.) accordingly.\n",
    "\n",
    "> **Hint**: Keep in mind that the best model is not always the most complex model, it should best fit with your data. You should start with a simple model as you did with the `baseline_model`, and increase the model complexity (number of neurons and hidden layers) gradually and when/if needed.\n",
    "\n",
    "> Recall that each time you want to run a new training session by calling `.fit()` method, you should re-run the build `nn_clf` and `compile` cells to re-start with a fresh random initilization of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366/366 - 1s - loss: 0.5608 - accuracy: 0.9900 - 1s/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate nn_clf on X_test, y_test\n",
    "# Required Test Accuracy: 0.99\n",
    "nn_clf_loss, nn_clf_test_accuracy = nn_clf.evaluate(X_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the Impact of Learning Rate on Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you're going to plot the impact of learning-rate on accuracy specifically. To do so and to avoid repeating the code, you should write a function `build_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(48, activation='LeakyReLU', input_shape=[48]), \n",
    "        tf.keras.layers.Dense(48, activation='LeakyReLU'),\n",
    "        tf.keras.layers.Dense(11, activation='softmax')])  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **learning rate range** that you're going to investigate is between [0.001, 0.01] inclusive with an increment step of 0.001. Use the optimizer and other hyperparatmeters of your choice that performed the best in your fine-tuning of `nn_clf` in the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rates range - the for loop of the following experiment will iterate over this array\n",
    "learning_rates = np.arange(0.001, 0.011, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009,\n",
       "       0.01 ])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(learning_rates))\n",
    "learning_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: For the following experiment, you should compile the model with `accuracy` as the metric, and use the same loss function that was used for `baseline_model` and `nn_clf` in each iteration of the `for` loop.\n",
    "\n",
    "> Running this cell may take a a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 1.1339 - accuracy: 0.6000 - val_loss: 0.6405 - val_accuracy: 0.7691\n",
      "Epoch 2/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.5431 - accuracy: 0.7972 - val_loss: 0.4522 - val_accuracy: 0.8317\n",
      "Epoch 3/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.4518 - accuracy: 0.8221 - val_loss: 0.4520 - val_accuracy: 0.8208\n",
      "Epoch 4/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.4124 - accuracy: 0.8338 - val_loss: 0.4013 - val_accuracy: 0.8280\n",
      "Epoch 5/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.3840 - accuracy: 0.8445 - val_loss: 0.3450 - val_accuracy: 0.8616\n",
      "Epoch 6/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.3676 - accuracy: 0.8513 - val_loss: 0.3980 - val_accuracy: 0.8297\n",
      "Epoch 7/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.3512 - accuracy: 0.8579 - val_loss: 0.3655 - val_accuracy: 0.8421\n",
      "Epoch 8/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.3444 - accuracy: 0.8593 - val_loss: 0.3166 - val_accuracy: 0.8682\n",
      "Epoch 9/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.3250 - accuracy: 0.8680 - val_loss: 0.2943 - val_accuracy: 0.8765\n",
      "Epoch 10/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.3146 - accuracy: 0.8717 - val_loss: 0.2844 - val_accuracy: 0.8784\n",
      "Epoch 11/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.3011 - accuracy: 0.8778 - val_loss: 0.2921 - val_accuracy: 0.8684\n",
      "Epoch 12/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.2926 - accuracy: 0.8822 - val_loss: 0.2721 - val_accuracy: 0.8881\n",
      "Epoch 13/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.2776 - accuracy: 0.8893 - val_loss: 0.2702 - val_accuracy: 0.8893\n",
      "Epoch 14/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.2718 - accuracy: 0.8904 - val_loss: 0.2835 - val_accuracy: 0.8855\n",
      "Epoch 15/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2601 - accuracy: 0.8956 - val_loss: 0.2623 - val_accuracy: 0.8930\n",
      "Epoch 16/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2515 - accuracy: 0.8999 - val_loss: 0.2186 - val_accuracy: 0.9109\n",
      "Epoch 17/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.2419 - accuracy: 0.9057 - val_loss: 0.2174 - val_accuracy: 0.9171\n",
      "Epoch 18/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2348 - accuracy: 0.9081 - val_loss: 0.2005 - val_accuracy: 0.9244\n",
      "Epoch 19/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2320 - accuracy: 0.9089 - val_loss: 0.1970 - val_accuracy: 0.9233\n",
      "Epoch 20/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.2228 - accuracy: 0.9140 - val_loss: 0.1965 - val_accuracy: 0.9246\n",
      "366/366 - 1s - loss: 0.2369 - accuracy: 0.9201 - 1s/epoch - 3ms/step\n",
      "Epoch 1/20\n",
      "1317/1317 [==============================] - 8s 5ms/step - loss: 0.8926 - accuracy: 0.6679 - val_loss: 0.5086 - val_accuracy: 0.7791\n",
      "Epoch 2/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.4676 - accuracy: 0.8109 - val_loss: 0.4491 - val_accuracy: 0.8150\n",
      "Epoch 3/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.4094 - accuracy: 0.8323 - val_loss: 0.3601 - val_accuracy: 0.8522\n",
      "Epoch 4/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.3713 - accuracy: 0.8486 - val_loss: 0.3596 - val_accuracy: 0.8490\n",
      "Epoch 5/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.3432 - accuracy: 0.8617 - val_loss: 0.3396 - val_accuracy: 0.8496\n",
      "Epoch 6/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.3201 - accuracy: 0.8720 - val_loss: 0.2929 - val_accuracy: 0.8774\n",
      "Epoch 7/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.2953 - accuracy: 0.8822 - val_loss: 0.2450 - val_accuracy: 0.9041\n",
      "Epoch 8/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.2939 - accuracy: 0.8820 - val_loss: 0.3851 - val_accuracy: 0.8323\n",
      "Epoch 9/20\n",
      "1317/1317 [==============================] - 7s 6ms/step - loss: 0.2691 - accuracy: 0.8939 - val_loss: 0.2469 - val_accuracy: 0.8970\n",
      "Epoch 10/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.2637 - accuracy: 0.8956 - val_loss: 0.2851 - val_accuracy: 0.8793\n",
      "Epoch 11/20\n",
      "1317/1317 [==============================] - 8s 6ms/step - loss: 0.2501 - accuracy: 0.9040 - val_loss: 0.2163 - val_accuracy: 0.9118\n",
      "Epoch 12/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.2354 - accuracy: 0.9113 - val_loss: 0.1811 - val_accuracy: 0.9363\n",
      "Epoch 13/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.2187 - accuracy: 0.9193 - val_loss: 0.2008 - val_accuracy: 0.9261\n",
      "Epoch 14/20\n",
      "1317/1317 [==============================] - 3s 3ms/step - loss: 0.2063 - accuracy: 0.9252 - val_loss: 0.2744 - val_accuracy: 0.8910\n",
      "Epoch 15/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1949 - accuracy: 0.9299 - val_loss: 0.1552 - val_accuracy: 0.9440\n",
      "Epoch 16/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1846 - accuracy: 0.9324 - val_loss: 0.2543 - val_accuracy: 0.8968\n",
      "Epoch 17/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.1799 - accuracy: 0.9350 - val_loss: 0.1333 - val_accuracy: 0.9566\n",
      "Epoch 18/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.1686 - accuracy: 0.9399 - val_loss: 0.1243 - val_accuracy: 0.9560\n",
      "Epoch 19/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.1655 - accuracy: 0.9413 - val_loss: 0.1813 - val_accuracy: 0.9299\n",
      "Epoch 20/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.1555 - accuracy: 0.9437 - val_loss: 0.1219 - val_accuracy: 0.9543\n",
      "366/366 - 1s - loss: 0.1685 - accuracy: 0.9492 - 554ms/epoch - 2ms/step\n",
      "Epoch 1/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.7337 - accuracy: 0.7173 - val_loss: 0.4569 - val_accuracy: 0.8285\n",
      "Epoch 2/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.4104 - accuracy: 0.8326 - val_loss: 0.3420 - val_accuracy: 0.8596\n",
      "Epoch 3/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.3569 - accuracy: 0.8564 - val_loss: 0.3350 - val_accuracy: 0.8588\n",
      "Epoch 4/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.3147 - accuracy: 0.8728 - val_loss: 0.2603 - val_accuracy: 0.8977\n",
      "Epoch 5/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2886 - accuracy: 0.8861 - val_loss: 0.2158 - val_accuracy: 0.9152\n",
      "Epoch 6/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.2676 - accuracy: 0.8941 - val_loss: 0.2140 - val_accuracy: 0.9150\n",
      "Epoch 7/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.2393 - accuracy: 0.9062 - val_loss: 0.1944 - val_accuracy: 0.9167\n",
      "Epoch 8/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.2232 - accuracy: 0.9151 - val_loss: 0.1672 - val_accuracy: 0.9327\n",
      "Epoch 9/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.2062 - accuracy: 0.9209 - val_loss: 0.1529 - val_accuracy: 0.9468\n",
      "Epoch 10/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.1979 - accuracy: 0.9238 - val_loss: 0.1437 - val_accuracy: 0.9481\n",
      "Epoch 11/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.1891 - accuracy: 0.9298 - val_loss: 0.1623 - val_accuracy: 0.9389\n",
      "Epoch 12/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.1873 - accuracy: 0.9299 - val_loss: 0.1426 - val_accuracy: 0.9404\n",
      "Epoch 13/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1712 - accuracy: 0.9363 - val_loss: 0.1469 - val_accuracy: 0.9400\n",
      "Epoch 14/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1649 - accuracy: 0.9375 - val_loss: 0.1716 - val_accuracy: 0.9316\n",
      "Epoch 15/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.1579 - accuracy: 0.9410 - val_loss: 0.1426 - val_accuracy: 0.9436\n",
      "Epoch 16/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1513 - accuracy: 0.9421 - val_loss: 0.1512 - val_accuracy: 0.9387\n",
      "Epoch 17/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1490 - accuracy: 0.9435 - val_loss: 0.0923 - val_accuracy: 0.9701\n",
      "Epoch 18/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1401 - accuracy: 0.9461 - val_loss: 0.1197 - val_accuracy: 0.9545\n",
      "Epoch 19/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1371 - accuracy: 0.9495 - val_loss: 0.1164 - val_accuracy: 0.9532\n",
      "Epoch 20/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.1297 - accuracy: 0.9521 - val_loss: 0.0816 - val_accuracy: 0.9729\n",
      "366/366 - 1s - loss: 0.1187 - accuracy: 0.9651 - 1s/epoch - 3ms/step\n",
      "Epoch 1/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.6970 - accuracy: 0.7245 - val_loss: 0.5518 - val_accuracy: 0.7496\n",
      "Epoch 2/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.4048 - accuracy: 0.8317 - val_loss: 0.3720 - val_accuracy: 0.8449\n",
      "Epoch 3/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.3449 - accuracy: 0.8601 - val_loss: 0.3005 - val_accuracy: 0.8684\n",
      "Epoch 4/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.3011 - accuracy: 0.8804 - val_loss: 0.2524 - val_accuracy: 0.8938\n",
      "Epoch 5/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2768 - accuracy: 0.8908 - val_loss: 0.2881 - val_accuracy: 0.8861\n",
      "Epoch 6/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2628 - accuracy: 0.8986 - val_loss: 0.1878 - val_accuracy: 0.9263\n",
      "Epoch 7/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2376 - accuracy: 0.9079 - val_loss: 0.2003 - val_accuracy: 0.9205\n",
      "Epoch 8/20\n",
      "1317/1317 [==============================] - 3s 3ms/step - loss: 0.2157 - accuracy: 0.9195 - val_loss: 0.2167 - val_accuracy: 0.9113\n",
      "Epoch 9/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2072 - accuracy: 0.9211 - val_loss: 0.1317 - val_accuracy: 0.9479\n",
      "Epoch 10/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1937 - accuracy: 0.9265 - val_loss: 0.1873 - val_accuracy: 0.9325\n",
      "Epoch 11/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1828 - accuracy: 0.9302 - val_loss: 0.1732 - val_accuracy: 0.9314\n",
      "Epoch 12/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1681 - accuracy: 0.9379 - val_loss: 0.1286 - val_accuracy: 0.9483\n",
      "Epoch 13/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1735 - accuracy: 0.9357 - val_loss: 0.1137 - val_accuracy: 0.9564\n",
      "Epoch 14/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1553 - accuracy: 0.9405 - val_loss: 0.1170 - val_accuracy: 0.9551\n",
      "Epoch 15/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1546 - accuracy: 0.9427 - val_loss: 0.1074 - val_accuracy: 0.9581\n",
      "Epoch 16/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1419 - accuracy: 0.9476 - val_loss: 0.2784 - val_accuracy: 0.9009\n",
      "Epoch 17/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1372 - accuracy: 0.9498 - val_loss: 0.1627 - val_accuracy: 0.9387\n",
      "Epoch 18/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1381 - accuracy: 0.9487 - val_loss: 0.1448 - val_accuracy: 0.9432\n",
      "Epoch 19/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1335 - accuracy: 0.9498 - val_loss: 0.0760 - val_accuracy: 0.9739\n",
      "Epoch 20/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1218 - accuracy: 0.9549 - val_loss: 0.1164 - val_accuracy: 0.9562\n",
      "366/366 - 0s - loss: 0.1623 - accuracy: 0.9571 - 477ms/epoch - 1ms/step\n",
      "Epoch 1/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.7204 - accuracy: 0.7111 - val_loss: 0.4015 - val_accuracy: 0.8411\n",
      "Epoch 2/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.4143 - accuracy: 0.8258 - val_loss: 0.3722 - val_accuracy: 0.8261\n",
      "Epoch 3/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.3588 - accuracy: 0.8527 - val_loss: 0.3396 - val_accuracy: 0.8479\n",
      "Epoch 4/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.3168 - accuracy: 0.8725 - val_loss: 0.2608 - val_accuracy: 0.8908\n",
      "Epoch 5/20\n",
      "1317/1317 [==============================] - 3s 3ms/step - loss: 0.2797 - accuracy: 0.8884 - val_loss: 0.2214 - val_accuracy: 0.9158\n",
      "Epoch 6/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2611 - accuracy: 0.8984 - val_loss: 0.3465 - val_accuracy: 0.8547\n",
      "Epoch 7/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2443 - accuracy: 0.9051 - val_loss: 0.2135 - val_accuracy: 0.9131\n",
      "Epoch 8/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2209 - accuracy: 0.9141 - val_loss: 0.2176 - val_accuracy: 0.9075\n",
      "Epoch 9/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.2063 - accuracy: 0.9218 - val_loss: 0.1434 - val_accuracy: 0.9427\n",
      "Epoch 10/20\n",
      "1317/1317 [==============================] - 3s 3ms/step - loss: 0.2032 - accuracy: 0.9232 - val_loss: 0.1396 - val_accuracy: 0.9436\n",
      "Epoch 11/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1886 - accuracy: 0.9297 - val_loss: 0.3052 - val_accuracy: 0.8752\n",
      "Epoch 12/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1787 - accuracy: 0.9338 - val_loss: 0.1224 - val_accuracy: 0.9564\n",
      "Epoch 13/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.1705 - accuracy: 0.9360 - val_loss: 0.0928 - val_accuracy: 0.9682\n",
      "Epoch 14/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1565 - accuracy: 0.9406 - val_loss: 0.3293 - val_accuracy: 0.8793\n",
      "Epoch 15/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.1562 - accuracy: 0.9419 - val_loss: 0.0918 - val_accuracy: 0.9667\n",
      "Epoch 16/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1521 - accuracy: 0.9439 - val_loss: 0.1404 - val_accuracy: 0.9449\n",
      "Epoch 17/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1430 - accuracy: 0.9461 - val_loss: 0.0979 - val_accuracy: 0.9662\n",
      "Epoch 18/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1462 - accuracy: 0.9468 - val_loss: 0.1000 - val_accuracy: 0.9605\n",
      "Epoch 19/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.1377 - accuracy: 0.9496 - val_loss: 0.1529 - val_accuracy: 0.9391\n",
      "Epoch 20/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1420 - accuracy: 0.9470 - val_loss: 0.0988 - val_accuracy: 0.9633\n",
      "366/366 - 1s - loss: 0.1496 - accuracy: 0.9627 - 1s/epoch - 3ms/step\n",
      "Epoch 1/20\n",
      "1317/1317 [==============================] - 8s 5ms/step - loss: 0.6969 - accuracy: 0.7095 - val_loss: 0.4507 - val_accuracy: 0.8073\n",
      "Epoch 2/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.4147 - accuracy: 0.8252 - val_loss: 0.3319 - val_accuracy: 0.8552\n",
      "Epoch 3/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.3467 - accuracy: 0.8561 - val_loss: 0.2595 - val_accuracy: 0.8992\n",
      "Epoch 4/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.3145 - accuracy: 0.8732 - val_loss: 0.3766 - val_accuracy: 0.8438\n",
      "Epoch 5/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.2800 - accuracy: 0.8892 - val_loss: 0.2667 - val_accuracy: 0.8930\n",
      "Epoch 6/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.2498 - accuracy: 0.9042 - val_loss: 0.2023 - val_accuracy: 0.9220\n",
      "Epoch 7/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.2183 - accuracy: 0.9169 - val_loss: 0.1442 - val_accuracy: 0.9425\n",
      "Epoch 8/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.2135 - accuracy: 0.9182 - val_loss: 0.2284 - val_accuracy: 0.9011\n",
      "Epoch 9/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.1995 - accuracy: 0.9222 - val_loss: 0.1444 - val_accuracy: 0.9455\n",
      "Epoch 10/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1845 - accuracy: 0.9299 - val_loss: 0.1408 - val_accuracy: 0.9447\n",
      "Epoch 11/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.1808 - accuracy: 0.9313 - val_loss: 0.3520 - val_accuracy: 0.8712\n",
      "Epoch 12/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1736 - accuracy: 0.9343 - val_loss: 0.1535 - val_accuracy: 0.9368\n",
      "Epoch 13/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1641 - accuracy: 0.9379 - val_loss: 0.1740 - val_accuracy: 0.9391\n",
      "Epoch 14/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1641 - accuracy: 0.9378 - val_loss: 0.3137 - val_accuracy: 0.8917\n",
      "Epoch 15/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1530 - accuracy: 0.9443 - val_loss: 0.1060 - val_accuracy: 0.9611\n",
      "Epoch 16/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.1610 - accuracy: 0.9423 - val_loss: 0.1352 - val_accuracy: 0.9434\n",
      "Epoch 17/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1455 - accuracy: 0.9470 - val_loss: 0.1855 - val_accuracy: 0.9248\n",
      "Epoch 18/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1364 - accuracy: 0.9475 - val_loss: 0.1115 - val_accuracy: 0.9596\n",
      "Epoch 19/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1396 - accuracy: 0.9480 - val_loss: 0.1140 - val_accuracy: 0.9564\n",
      "Epoch 20/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.1307 - accuracy: 0.9518 - val_loss: 0.0768 - val_accuracy: 0.9729\n",
      "366/366 - 1s - loss: 0.1621 - accuracy: 0.9623 - 1s/epoch - 3ms/step\n",
      "Epoch 1/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.6909 - accuracy: 0.7183 - val_loss: 0.4460 - val_accuracy: 0.7996\n",
      "Epoch 2/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.4014 - accuracy: 0.8298 - val_loss: 0.3579 - val_accuracy: 0.8535\n",
      "Epoch 3/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.3644 - accuracy: 0.8500 - val_loss: 0.2734 - val_accuracy: 0.8772\n",
      "Epoch 4/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.3119 - accuracy: 0.8736 - val_loss: 0.2344 - val_accuracy: 0.9088\n",
      "Epoch 5/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.2798 - accuracy: 0.8876 - val_loss: 0.2117 - val_accuracy: 0.9145\n",
      "Epoch 6/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.2707 - accuracy: 0.8926 - val_loss: 0.1964 - val_accuracy: 0.9278\n",
      "Epoch 7/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.2470 - accuracy: 0.9017 - val_loss: 0.1844 - val_accuracy: 0.9267\n",
      "Epoch 8/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.2283 - accuracy: 0.9119 - val_loss: 0.2503 - val_accuracy: 0.8915\n",
      "Epoch 9/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.2233 - accuracy: 0.9144 - val_loss: 0.2119 - val_accuracy: 0.9084\n",
      "Epoch 10/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.2131 - accuracy: 0.9171 - val_loss: 0.1343 - val_accuracy: 0.9483\n",
      "Epoch 11/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.1980 - accuracy: 0.9262 - val_loss: 0.1493 - val_accuracy: 0.9387\n",
      "Epoch 12/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1939 - accuracy: 0.9258 - val_loss: 0.1468 - val_accuracy: 0.9455\n",
      "Epoch 13/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1766 - accuracy: 0.9335 - val_loss: 0.1934 - val_accuracy: 0.9291\n",
      "Epoch 14/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1860 - accuracy: 0.9306 - val_loss: 0.1377 - val_accuracy: 0.9507\n",
      "Epoch 15/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.1735 - accuracy: 0.9376 - val_loss: 0.2151 - val_accuracy: 0.9022\n",
      "Epoch 16/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1626 - accuracy: 0.9395 - val_loss: 0.1240 - val_accuracy: 0.9530\n",
      "Epoch 17/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1795 - accuracy: 0.9357 - val_loss: 0.1015 - val_accuracy: 0.9581\n",
      "Epoch 18/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1445 - accuracy: 0.9461 - val_loss: 0.1714 - val_accuracy: 0.9357\n",
      "Epoch 19/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.1396 - accuracy: 0.9494 - val_loss: 0.1054 - val_accuracy: 0.9568\n",
      "Epoch 20/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1524 - accuracy: 0.9447 - val_loss: 0.2546 - val_accuracy: 0.9017\n",
      "366/366 - 1s - loss: 0.3457 - accuracy: 0.9075 - 1s/epoch - 3ms/step\n",
      "Epoch 1/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.6759 - accuracy: 0.7192 - val_loss: 0.4115 - val_accuracy: 0.8163\n",
      "Epoch 2/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.4252 - accuracy: 0.8197 - val_loss: 0.3476 - val_accuracy: 0.8344\n",
      "Epoch 3/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.3480 - accuracy: 0.8556 - val_loss: 0.3020 - val_accuracy: 0.8729\n",
      "Epoch 4/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.3201 - accuracy: 0.8711 - val_loss: 0.3493 - val_accuracy: 0.8515\n",
      "Epoch 5/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.3012 - accuracy: 0.8795 - val_loss: 0.3536 - val_accuracy: 0.8492\n",
      "Epoch 6/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2726 - accuracy: 0.8930 - val_loss: 0.1933 - val_accuracy: 0.9178\n",
      "Epoch 7/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2705 - accuracy: 0.8948 - val_loss: 0.3427 - val_accuracy: 0.8590\n",
      "Epoch 8/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2434 - accuracy: 0.9059 - val_loss: 0.2203 - val_accuracy: 0.9122\n",
      "Epoch 9/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2383 - accuracy: 0.9097 - val_loss: 0.1568 - val_accuracy: 0.9393\n",
      "Epoch 10/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2106 - accuracy: 0.9180 - val_loss: 0.1856 - val_accuracy: 0.9269\n",
      "Epoch 11/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2123 - accuracy: 0.9200 - val_loss: 0.1931 - val_accuracy: 0.9182\n",
      "Epoch 12/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.2049 - accuracy: 0.9216 - val_loss: 0.1471 - val_accuracy: 0.9457\n",
      "Epoch 13/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.2021 - accuracy: 0.9233 - val_loss: 0.1636 - val_accuracy: 0.9357\n",
      "Epoch 14/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1879 - accuracy: 0.9289 - val_loss: 0.1602 - val_accuracy: 0.9319\n",
      "Epoch 15/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.1922 - accuracy: 0.9284 - val_loss: 0.1364 - val_accuracy: 0.9453\n",
      "Epoch 16/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1802 - accuracy: 0.9332 - val_loss: 0.1342 - val_accuracy: 0.9413\n",
      "Epoch 17/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.1787 - accuracy: 0.9341 - val_loss: 0.3657 - val_accuracy: 0.8740\n",
      "Epoch 18/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1823 - accuracy: 0.9340 - val_loss: 0.2838 - val_accuracy: 0.8943\n",
      "Epoch 19/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1734 - accuracy: 0.9402 - val_loss: 0.1808 - val_accuracy: 0.9344\n",
      "Epoch 20/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.1720 - accuracy: 0.9383 - val_loss: 0.1244 - val_accuracy: 0.9487\n",
      "366/366 - 1s - loss: 0.1933 - accuracy: 0.9516 - 1s/epoch - 3ms/step\n",
      "Epoch 1/20\n",
      "1317/1317 [==============================] - 8s 5ms/step - loss: 0.6502 - accuracy: 0.7267 - val_loss: 0.4083 - val_accuracy: 0.8289\n",
      "Epoch 2/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.4175 - accuracy: 0.8235 - val_loss: 0.4412 - val_accuracy: 0.8005\n",
      "Epoch 3/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.3742 - accuracy: 0.8494 - val_loss: 0.2522 - val_accuracy: 0.8981\n",
      "Epoch 4/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.3045 - accuracy: 0.8774 - val_loss: 0.2350 - val_accuracy: 0.9000\n",
      "Epoch 5/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.2896 - accuracy: 0.8834 - val_loss: 0.3806 - val_accuracy: 0.8479\n",
      "Epoch 6/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.2747 - accuracy: 0.8893 - val_loss: 0.2115 - val_accuracy: 0.9199\n",
      "Epoch 7/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.2476 - accuracy: 0.9018 - val_loss: 0.2222 - val_accuracy: 0.9007\n",
      "Epoch 8/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.2478 - accuracy: 0.9027 - val_loss: 0.1745 - val_accuracy: 0.9314\n",
      "Epoch 9/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.2325 - accuracy: 0.9102 - val_loss: 0.2455 - val_accuracy: 0.8985\n",
      "Epoch 10/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.2235 - accuracy: 0.9151 - val_loss: 0.1519 - val_accuracy: 0.9372\n",
      "Epoch 11/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.2150 - accuracy: 0.9178 - val_loss: 0.2644 - val_accuracy: 0.8913\n",
      "Epoch 12/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.2010 - accuracy: 0.9227 - val_loss: 0.3224 - val_accuracy: 0.8793\n",
      "Epoch 13/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.2015 - accuracy: 0.9248 - val_loss: 0.2264 - val_accuracy: 0.9032\n",
      "Epoch 14/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1875 - accuracy: 0.9312 - val_loss: 0.2734 - val_accuracy: 0.8992\n",
      "Epoch 15/20\n",
      "1317/1317 [==============================] - 3s 3ms/step - loss: 0.1988 - accuracy: 0.9263 - val_loss: 0.1639 - val_accuracy: 0.9370\n",
      "Epoch 16/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.1802 - accuracy: 0.9331 - val_loss: 0.1194 - val_accuracy: 0.9519\n",
      "Epoch 17/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.1715 - accuracy: 0.9360 - val_loss: 0.1377 - val_accuracy: 0.9468\n",
      "Epoch 18/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1782 - accuracy: 0.9335 - val_loss: 0.2686 - val_accuracy: 0.9037\n",
      "Epoch 19/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1801 - accuracy: 0.9323 - val_loss: 0.1288 - val_accuracy: 0.9470\n",
      "Epoch 20/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.1678 - accuracy: 0.9394 - val_loss: 0.1985 - val_accuracy: 0.9282\n",
      "366/366 - 0s - loss: 0.3063 - accuracy: 0.9277 - 491ms/epoch - 1ms/step\n",
      "Epoch 1/20\n",
      "1317/1317 [==============================] - 8s 5ms/step - loss: 0.6805 - accuracy: 0.7140 - val_loss: 0.3989 - val_accuracy: 0.8351\n",
      "Epoch 2/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.4069 - accuracy: 0.8265 - val_loss: 0.4360 - val_accuracy: 0.8253\n",
      "Epoch 3/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.3807 - accuracy: 0.8397 - val_loss: 0.3494 - val_accuracy: 0.8628\n",
      "Epoch 4/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.3304 - accuracy: 0.8642 - val_loss: 0.7156 - val_accuracy: 0.7759\n",
      "Epoch 5/20\n",
      "1317/1317 [==============================] - 4s 3ms/step - loss: 0.3216 - accuracy: 0.8694 - val_loss: 0.5430 - val_accuracy: 0.7774\n",
      "Epoch 6/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.3016 - accuracy: 0.8800 - val_loss: 0.2445 - val_accuracy: 0.9028\n",
      "Epoch 7/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.2747 - accuracy: 0.8900 - val_loss: 0.2192 - val_accuracy: 0.9124\n",
      "Epoch 8/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.2842 - accuracy: 0.8892 - val_loss: 0.2508 - val_accuracy: 0.8893\n",
      "Epoch 9/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.2489 - accuracy: 0.9017 - val_loss: 0.3895 - val_accuracy: 0.8552\n",
      "Epoch 10/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.2355 - accuracy: 0.9089 - val_loss: 0.2121 - val_accuracy: 0.9086\n",
      "Epoch 11/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2614 - accuracy: 0.9037 - val_loss: 0.4679 - val_accuracy: 0.8317\n",
      "Epoch 12/20\n",
      "1317/1317 [==============================] - 3s 2ms/step - loss: 0.2169 - accuracy: 0.9182 - val_loss: 0.1718 - val_accuracy: 0.9291\n",
      "Epoch 13/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.2042 - accuracy: 0.9239 - val_loss: 0.4236 - val_accuracy: 0.8406\n",
      "Epoch 14/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.2201 - accuracy: 0.9189 - val_loss: 0.1373 - val_accuracy: 0.9472\n",
      "Epoch 15/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1954 - accuracy: 0.9270 - val_loss: 0.2605 - val_accuracy: 0.9034\n",
      "Epoch 16/20\n",
      "1317/1317 [==============================] - 5s 4ms/step - loss: 0.2135 - accuracy: 0.9210 - val_loss: 0.1209 - val_accuracy: 0.9556\n",
      "Epoch 17/20\n",
      "1317/1317 [==============================] - 6s 4ms/step - loss: 0.1933 - accuracy: 0.9302 - val_loss: 0.0842 - val_accuracy: 0.9690\n",
      "Epoch 18/20\n",
      "1317/1317 [==============================] - 6s 5ms/step - loss: 0.1901 - accuracy: 0.9319 - val_loss: 0.1470 - val_accuracy: 0.9374\n",
      "Epoch 19/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1870 - accuracy: 0.9319 - val_loss: 0.1148 - val_accuracy: 0.9577\n",
      "Epoch 20/20\n",
      "1317/1317 [==============================] - 7s 5ms/step - loss: 0.1783 - accuracy: 0.9347 - val_loss: 0.1295 - val_accuracy: 0.9483\n",
      "366/366 - 1s - loss: 0.2157 - accuracy: 0.9485 - 1s/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# accuracies list\n",
    "accuracies = []\n",
    "\n",
    "''' Write a for loop that iterates over learning_rates array which was defined in the previous cell\n",
    "    In each iteration:\n",
    "        build a new model by calling build_model(),\n",
    "        compile with the optimizer of your choice, and the current learning_rate in this iteration,\n",
    "        train on X_train and y_train with 20 epochs,\n",
    "        evaluate the model on X_test, y_test and get the test accuracy,\n",
    "        append the test accuracy to the accuracies list\n",
    "'''\n",
    "\n",
    "for rate in learning_rates:\n",
    "    rate_model = build_model()\n",
    "    rate_model.compile(loss=\"categorical_crossentropy\", metrics=['accuracy'], optimizer=keras.optimizers.Adam(learning_rate=rate))\n",
    "    rate_model_history = rate_model.fit(X_train, y_train, epochs=20, validation_split=0.1)\n",
    "    rate_model_loss, rate_model_test_accuracy = rate_model.evaluate(X_test,  y_test, verbose=2)\n",
    "    accuracies.append(rate_model_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' [20 Points]\n",
    "    Plot the \"Impact of Learning Rate on Accuracy\" with 'Learning Rate' on x-axis and 'Accuracy' on y_axis\n",
    "    The plot should use 'seaborn' style and with a figsize=(10, 5) and fontsize=14 for the title/labels\n",
    "    The plot must have title, axis labels, and xticks precisely as specified and displayed below\n",
    "    \n",
    "    The range of your accuracies may slightly differ but should be similar, and\n",
    "    the range of learning rate should be exactly the same, i.e from 0.001 to 0.01\n",
    "    \n",
    "    Incomplete/wrong plots will get no credit '''\n",
    "\n",
    "seaborn.set_theme()\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(learning_rates, accuracies)\n",
    "plt.title('Impact of Learning Rate on Accuracy', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=14)\n",
    "plt.xticks(learning_rates)\n",
    "plt.xlabel('Learning Rate', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-I - Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER THE FOLLOWING QUESTIONS HERE:\n",
    "\n",
    "**Q1 [5 points]** - According to your plot of accuracy vs learning rate, what is a good value for learning rate? **Why**? Mention the reason clearly.\n",
    "\n",
    "Answer: \n",
    "\n",
    "According to the graph 0.003 seems to give the best results in this instance. The and the mid-lower range of learning rate seem to do better then the upper high and lowest range. This is because too high of a learning rate will cause the optimizer to take large steps and often overshoot and bounce around the optimal minimum but never converge. And too small of a learning rate will take much longer to converge and my take to long to do so. So in theis inctance the mid range of learning rate seem to do better than the outer range of learning rates  \n",
    "\n",
    "**Q2 [5 points]** - Name five learning rate scheduling policies known as **learning schedules** (you may consult with the textbook), and explain each briefly in no more than two sentences here.\n",
    "\n",
    "Answer (each policy with explanation has 1 point):\n",
    "\n",
    "**Power scheduling:** Power scheduling starts at a inital specifyed learning rate Î· and after S steps it will drop the learning rate by dividing it some increasing number. Power schedualing first drops quickly, then more and more slowly.\n",
    "\n",
    "\n",
    "**Exponential scheduling:** The learning rate will gradually drop by a specifyed factor x every s steps. unlike power schedualing where its rate drops more slowly as it goes, in exponetial scheduling the rate keeps getting slashed by the specifyed factor every s steps.\n",
    "\n",
    "**Piecewise constant scheduling:** Piecewise constant Scheduling will use a constant learning rate for a number of epochs and then another constant rate for the next number of epochs and so on. \n",
    "\n",
    "**Performance scheduling:** This will Measure the validation error every N steps. and reduce the learning rate by a certan factor unti the error stops dropping \n",
    "\n",
    "**1cycle scheduling:**  This starts by Increasing the initial learning rate $Î·_0$, growing it up to $Î·_1$ halfway through training. Then decreases the rate down back to $Î·_0$ during the second half of training. and the last few epochs will have the rate reduced more by several orders of magnitude. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-II - Regression Using NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Part-II, you're going to perform a regression task using NN that you build in Tensorflow/Keras framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the [\"Video Game Sales with Ratings\"](https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings) dataset and read the descriptions on the Kaggle page. You are going to build and train a regression NN to predict **`NA_Sales`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16719, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>NA_Sales</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Global_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Score</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Developer</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wii Sports</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>41.36</td>\n",
       "      <td>28.96</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.45</td>\n",
       "      <td>82.53</td>\n",
       "      <td>76.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8</td>\n",
       "      <td>322.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Super Mario Bros.</td>\n",
       "      <td>NES</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>Platform</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>29.08</td>\n",
       "      <td>3.58</td>\n",
       "      <td>6.81</td>\n",
       "      <td>0.77</td>\n",
       "      <td>40.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mario Kart Wii</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>Racing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.68</td>\n",
       "      <td>12.76</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.29</td>\n",
       "      <td>35.52</td>\n",
       "      <td>82.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>709.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wii Sports Resort</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.61</td>\n",
       "      <td>10.93</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>32.77</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8</td>\n",
       "      <td>192.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pokemon Red/Pokemon Blue</td>\n",
       "      <td>GB</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>Role-Playing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>11.27</td>\n",
       "      <td>8.89</td>\n",
       "      <td>10.22</td>\n",
       "      <td>1.00</td>\n",
       "      <td>31.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name Platform  Year_of_Release         Genre Publisher  \\\n",
       "0                Wii Sports      Wii           2006.0        Sports  Nintendo   \n",
       "1         Super Mario Bros.      NES           1985.0      Platform  Nintendo   \n",
       "2            Mario Kart Wii      Wii           2008.0        Racing  Nintendo   \n",
       "3         Wii Sports Resort      Wii           2009.0        Sports  Nintendo   \n",
       "4  Pokemon Red/Pokemon Blue       GB           1996.0  Role-Playing  Nintendo   \n",
       "\n",
       "   NA_Sales  EU_Sales  JP_Sales  Other_Sales  Global_Sales  Critic_Score  \\\n",
       "0     41.36     28.96      3.77         8.45         82.53          76.0   \n",
       "1     29.08      3.58      6.81         0.77         40.24           NaN   \n",
       "2     15.68     12.76      3.79         3.29         35.52          82.0   \n",
       "3     15.61     10.93      3.28         2.95         32.77          80.0   \n",
       "4     11.27      8.89     10.22         1.00         31.37           NaN   \n",
       "\n",
       "   Critic_Count User_Score  User_Count Developer Rating  \n",
       "0          51.0          8       322.0  Nintendo      E  \n",
       "1           NaN        NaN         NaN       NaN    NaN  \n",
       "2          73.0        8.3       709.0  Nintendo      E  \n",
       "3          73.0          8       192.0  Nintendo      E  \n",
       "4           NaN        NaN         NaN       NaN    NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the 'Video_Games_Sales_as_at_22_Dec_2016.csv' as a dataframe using pandas\n",
    "data2 = pd.read_csv('Video_Games_Sales_as_at_22_Dec_2016.csv')\n",
    "print(data2.shape)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the multiple steps of preprocessing very carefully.\n",
    "\n",
    "**NOTE**: If you do not perform the preprocessing steps correctly, all of your results would be wrong and your Part-II will get zero points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As usual, check if there is any NAs (there are plenty) and drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name                  2\n",
       "Platform              0\n",
       "Year_of_Release     269\n",
       "Genre                 2\n",
       "Publisher            54\n",
       "NA_Sales              0\n",
       "EU_Sales              0\n",
       "JP_Sales              0\n",
       "Other_Sales           0\n",
       "Global_Sales          0\n",
       "Critic_Score       8582\n",
       "Critic_Count       8582\n",
       "User_Score         6704\n",
       "User_Count         9129\n",
       "Developer          6623\n",
       "Rating             6769\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check NAs\n",
    "data2.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6825, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>NA_Sales</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Global_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Score</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Developer</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wii Sports</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>41.36</td>\n",
       "      <td>28.96</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.45</td>\n",
       "      <td>82.53</td>\n",
       "      <td>76.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8</td>\n",
       "      <td>322.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mario Kart Wii</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>Racing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.68</td>\n",
       "      <td>12.76</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.29</td>\n",
       "      <td>35.52</td>\n",
       "      <td>82.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>709.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wii Sports Resort</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.61</td>\n",
       "      <td>10.93</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>32.77</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8</td>\n",
       "      <td>192.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>New Super Mario Bros.</td>\n",
       "      <td>DS</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Platform</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>11.28</td>\n",
       "      <td>9.14</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2.88</td>\n",
       "      <td>29.80</td>\n",
       "      <td>89.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>431.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wii Play</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Misc</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>13.96</td>\n",
       "      <td>9.18</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.84</td>\n",
       "      <td>28.92</td>\n",
       "      <td>58.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>129.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name Platform  Year_of_Release     Genre Publisher  \\\n",
       "0             Wii Sports      Wii           2006.0    Sports  Nintendo   \n",
       "2         Mario Kart Wii      Wii           2008.0    Racing  Nintendo   \n",
       "3      Wii Sports Resort      Wii           2009.0    Sports  Nintendo   \n",
       "6  New Super Mario Bros.       DS           2006.0  Platform  Nintendo   \n",
       "7               Wii Play      Wii           2006.0      Misc  Nintendo   \n",
       "\n",
       "   NA_Sales  EU_Sales  JP_Sales  Other_Sales  Global_Sales  Critic_Score  \\\n",
       "0     41.36     28.96      3.77         8.45         82.53          76.0   \n",
       "2     15.68     12.76      3.79         3.29         35.52          82.0   \n",
       "3     15.61     10.93      3.28         2.95         32.77          80.0   \n",
       "6     11.28      9.14      6.50         2.88         29.80          89.0   \n",
       "7     13.96      9.18      2.93         2.84         28.92          58.0   \n",
       "\n",
       "   Critic_Count User_Score  User_Count Developer Rating  \n",
       "0          51.0          8       322.0  Nintendo      E  \n",
       "2          73.0        8.3       709.0  Nintendo      E  \n",
       "3          73.0          8       192.0  Nintendo      E  \n",
       "6          65.0        8.5       431.0  Nintendo      E  \n",
       "7          41.0        6.6       129.0  Nintendo      E  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop NAs\n",
    "data2 = data2.dropna()\n",
    "print(data2.shape)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name               0\n",
       "Platform           0\n",
       "Year_of_Release    0\n",
       "Genre              0\n",
       "Publisher          0\n",
       "NA_Sales           0\n",
       "EU_Sales           0\n",
       "JP_Sales           0\n",
       "Other_Sales        0\n",
       "Global_Sales       0\n",
       "Critic_Score       0\n",
       "Critic_Count       0\n",
       "User_Score         0\n",
       "User_Count         0\n",
       "Developer          0\n",
       "Rating             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check NAs again\n",
    "data2.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since there were a lot of NAs, you've missed a lot of indexes of the dropped rows after dropping NAs, so you should use [`reset_index()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) but no new column should be added as `index`, so you should set the `drop` parameter of `reset_index()` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6825, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>NA_Sales</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Global_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Score</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Developer</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wii Sports</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>41.36</td>\n",
       "      <td>28.96</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.45</td>\n",
       "      <td>82.53</td>\n",
       "      <td>76.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8</td>\n",
       "      <td>322.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mario Kart Wii</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>Racing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.68</td>\n",
       "      <td>12.76</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.29</td>\n",
       "      <td>35.52</td>\n",
       "      <td>82.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>709.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wii Sports Resort</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.61</td>\n",
       "      <td>10.93</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>32.77</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8</td>\n",
       "      <td>192.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>New Super Mario Bros.</td>\n",
       "      <td>DS</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Platform</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>11.28</td>\n",
       "      <td>9.14</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2.88</td>\n",
       "      <td>29.80</td>\n",
       "      <td>89.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>431.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wii Play</td>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Misc</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>13.96</td>\n",
       "      <td>9.18</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.84</td>\n",
       "      <td>28.92</td>\n",
       "      <td>58.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>129.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name Platform  Year_of_Release     Genre Publisher  \\\n",
       "0             Wii Sports      Wii           2006.0    Sports  Nintendo   \n",
       "1         Mario Kart Wii      Wii           2008.0    Racing  Nintendo   \n",
       "2      Wii Sports Resort      Wii           2009.0    Sports  Nintendo   \n",
       "3  New Super Mario Bros.       DS           2006.0  Platform  Nintendo   \n",
       "4               Wii Play      Wii           2006.0      Misc  Nintendo   \n",
       "\n",
       "   NA_Sales  EU_Sales  JP_Sales  Other_Sales  Global_Sales  Critic_Score  \\\n",
       "0     41.36     28.96      3.77         8.45         82.53          76.0   \n",
       "1     15.68     12.76      3.79         3.29         35.52          82.0   \n",
       "2     15.61     10.93      3.28         2.95         32.77          80.0   \n",
       "3     11.28      9.14      6.50         2.88         29.80          89.0   \n",
       "4     13.96      9.18      2.93         2.84         28.92          58.0   \n",
       "\n",
       "   Critic_Count User_Score  User_Count Developer Rating  \n",
       "0          51.0          8       322.0  Nintendo      E  \n",
       "1          73.0        8.3       709.0  Nintendo      E  \n",
       "2          73.0          8       192.0  Nintendo      E  \n",
       "3          65.0        8.5       431.0  Nintendo      E  \n",
       "4          41.0        6.6       129.0  Nintendo      E  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset index so that the rows are indexed from 0 and increment by one, no column should be added!\n",
    "data2 = data2.reset_index(drop=True)\n",
    "print(data2.shape)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6825, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>NA_Sales</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Global_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Score</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Developer</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>41.36</td>\n",
       "      <td>28.96</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.45</td>\n",
       "      <td>82.53</td>\n",
       "      <td>76.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8</td>\n",
       "      <td>322.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>Racing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.68</td>\n",
       "      <td>12.76</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.29</td>\n",
       "      <td>35.52</td>\n",
       "      <td>82.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>709.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.61</td>\n",
       "      <td>10.93</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>32.77</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8</td>\n",
       "      <td>192.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DS</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Platform</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>11.28</td>\n",
       "      <td>9.14</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2.88</td>\n",
       "      <td>29.80</td>\n",
       "      <td>89.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>431.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Misc</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>13.96</td>\n",
       "      <td>9.18</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.84</td>\n",
       "      <td>28.92</td>\n",
       "      <td>58.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>129.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Platform  Year_of_Release     Genre Publisher  NA_Sales  EU_Sales  JP_Sales  \\\n",
       "0      Wii           2006.0    Sports  Nintendo     41.36     28.96      3.77   \n",
       "1      Wii           2008.0    Racing  Nintendo     15.68     12.76      3.79   \n",
       "2      Wii           2009.0    Sports  Nintendo     15.61     10.93      3.28   \n",
       "3       DS           2006.0  Platform  Nintendo     11.28      9.14      6.50   \n",
       "4      Wii           2006.0      Misc  Nintendo     13.96      9.18      2.93   \n",
       "\n",
       "   Other_Sales  Global_Sales  Critic_Score  Critic_Count User_Score  \\\n",
       "0         8.45         82.53          76.0          51.0          8   \n",
       "1         3.29         35.52          82.0          73.0        8.3   \n",
       "2         2.95         32.77          80.0          73.0          8   \n",
       "3         2.88         29.80          89.0          65.0        8.5   \n",
       "4         2.84         28.92          58.0          41.0        6.6   \n",
       "\n",
       "   User_Count Developer Rating  \n",
       "0       322.0  Nintendo      E  \n",
       "1       709.0  Nintendo      E  \n",
       "2       192.0  Nintendo      E  \n",
       "3       431.0  Nintendo      E  \n",
       "4       129.0  Nintendo      E  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop \"Name\" column as it does not provide useful info for your model training\n",
    "data2 = data2.drop(columns=\"Name\")\n",
    "print(data2.shape)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `NA_Sales` is the target column for prediction; however, since the column `Global_Sales` is the sum of other sales, you should drop it; otherwise, it corrupts the training process by leaking information and violating the regression assumption that features are independent.\n",
    "\n",
    "> **NOTE**: Make sure to match your `data2` dataframe with the provided outputs after every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6825, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>NA_Sales</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Score</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Developer</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>41.36</td>\n",
       "      <td>28.96</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.45</td>\n",
       "      <td>76.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8</td>\n",
       "      <td>322.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>Racing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.68</td>\n",
       "      <td>12.76</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.29</td>\n",
       "      <td>82.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>709.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>15.61</td>\n",
       "      <td>10.93</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8</td>\n",
       "      <td>192.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DS</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Platform</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>11.28</td>\n",
       "      <td>9.14</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2.88</td>\n",
       "      <td>89.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>431.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Misc</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>13.96</td>\n",
       "      <td>9.18</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.84</td>\n",
       "      <td>58.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>129.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Platform  Year_of_Release     Genre Publisher  NA_Sales  EU_Sales  JP_Sales  \\\n",
       "0      Wii           2006.0    Sports  Nintendo     41.36     28.96      3.77   \n",
       "1      Wii           2008.0    Racing  Nintendo     15.68     12.76      3.79   \n",
       "2      Wii           2009.0    Sports  Nintendo     15.61     10.93      3.28   \n",
       "3       DS           2006.0  Platform  Nintendo     11.28      9.14      6.50   \n",
       "4      Wii           2006.0      Misc  Nintendo     13.96      9.18      2.93   \n",
       "\n",
       "   Other_Sales  Critic_Score  Critic_Count User_Score  User_Count Developer  \\\n",
       "0         8.45          76.0          51.0          8       322.0  Nintendo   \n",
       "1         3.29          82.0          73.0        8.3       709.0  Nintendo   \n",
       "2         2.95          80.0          73.0          8       192.0  Nintendo   \n",
       "3         2.88          89.0          65.0        8.5       431.0  Nintendo   \n",
       "4         2.84          58.0          41.0        6.6       129.0  Nintendo   \n",
       "\n",
       "  Rating  \n",
       "0      E  \n",
       "1      E  \n",
       "2      E  \n",
       "3      E  \n",
       "4      E  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop \"Global_Sales\" column\n",
    "data2 = data2.drop(columns=\"Global_Sales\")\n",
    "print(data2.shape)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, check the statistical description of `NA_Sales`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6825.000000\n",
       "mean        0.394484\n",
       "std         0.967385\n",
       "min         0.000000\n",
       "25%         0.060000\n",
       "50%         0.150000\n",
       "75%         0.390000\n",
       "max        41.360000\n",
       "Name: NA_Sales, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2['NA_Sales'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year_of_Release   -0.016239\n",
       "Critic_Score       0.233580\n",
       "User_Count         0.246208\n",
       "Critic_Count       0.283917\n",
       "JP_Sales           0.468607\n",
       "Other_Sales        0.726757\n",
       "EU_Sales           0.841808\n",
       "NA_Sales           1.000000\n",
       "Name: NA_Sales, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find correlations with NA_Sales\n",
    "correlations = data2.corr()[\"NA_Sales\"].sort_values()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Separate features from the target column, so `X2` should contain all columns except `NA_Sales`. `y2` should contain `NA_Sales` only. `X2` and `y2` are so named to differentiate them from the features and labels of Part-I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature vector X2 (all columns but \"NA_Sales\") and target label y2 as \"NA_Sales\"\n",
    "X2 = data2.drop(columns='NA_Sales')\n",
    "y2 = data2['NA_Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6825, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Score</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Developer</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>28.96</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.45</td>\n",
       "      <td>76.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8</td>\n",
       "      <td>322.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>Racing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>12.76</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.29</td>\n",
       "      <td>82.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>709.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>10.93</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8</td>\n",
       "      <td>192.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DS</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Platform</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>9.14</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2.88</td>\n",
       "      <td>89.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>431.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Misc</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>9.18</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.84</td>\n",
       "      <td>58.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>129.0</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Platform  Year_of_Release     Genre Publisher  EU_Sales  JP_Sales  \\\n",
       "0      Wii           2006.0    Sports  Nintendo     28.96      3.77   \n",
       "1      Wii           2008.0    Racing  Nintendo     12.76      3.79   \n",
       "2      Wii           2009.0    Sports  Nintendo     10.93      3.28   \n",
       "3       DS           2006.0  Platform  Nintendo      9.14      6.50   \n",
       "4      Wii           2006.0      Misc  Nintendo      9.18      2.93   \n",
       "\n",
       "   Other_Sales  Critic_Score  Critic_Count User_Score  User_Count Developer  \\\n",
       "0         8.45          76.0          51.0          8       322.0  Nintendo   \n",
       "1         3.29          82.0          73.0        8.3       709.0  Nintendo   \n",
       "2         2.95          80.0          73.0          8       192.0  Nintendo   \n",
       "3         2.88          89.0          65.0        8.5       431.0  Nintendo   \n",
       "4         2.84          58.0          41.0        6.6       129.0  Nintendo   \n",
       "\n",
       "  Rating  \n",
       "0      E  \n",
       "1      E  \n",
       "2      E  \n",
       "3      E  \n",
       "4      E  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print X2 shape and head\n",
    "print(X2.shape)\n",
    "X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6825,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    41.36\n",
       "1    15.68\n",
       "2    15.61\n",
       "3    11.28\n",
       "4    13.96\n",
       "Name: NA_Sales, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print y2 shape and head\n",
    "print(y2.shape)\n",
    "y2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You should convert `object` (categorical) features, and there are multiple strategies for encoding categorical features. The right choice depends on the feature and number of unique values it has. Below you see some of those strategies applied on different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Platform            object\n",
       "Year_of_Release    float64\n",
       "Genre               object\n",
       "Publisher           object\n",
       "EU_Sales           float64\n",
       "JP_Sales           float64\n",
       "Other_Sales        float64\n",
       "Critic_Score       float64\n",
       "Critic_Count       float64\n",
       "User_Score          object\n",
       "User_Count         float64\n",
       "Developer           object\n",
       "Rating              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since `Developer` is categorical, we should encode it. The issue is that it has 1289 unique values, so **one-hot encoding** is not a good choice due to **curse of dimensionality**. Also, the categorical values have no ranking, meaning `Developer` is a **nominal variable**; thus, **Label Encoding** is NOT a good choice either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1289"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.Developer.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Frequency Encoding** seems a good choice for `Developer` although we definitely lose some information, but still it's better than **One-Hot Encoding** and **Label Encoding** for this particular feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts = X2.Developer.value_counts().to_frame()\n",
    "val_counts.rename(columns={\"vendor_name\": \"count\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Developer</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EA Canada</th>\n",
       "      <td>149</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EA Sports</th>\n",
       "      <td>142</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Capcom</th>\n",
       "      <td>126</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ubisoft</th>\n",
       "      <td>103</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Konami</th>\n",
       "      <td>95</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sanzaru Games, Sanzaru Games, Inc.</th>\n",
       "      <td>1</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCEA, Think and Feel</th>\n",
       "      <td>1</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ubisoft Annecy</th>\n",
       "      <td>1</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omega Force, Koei Canada</th>\n",
       "      <td>1</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atomic Games</th>\n",
       "      <td>1</td>\n",
       "      <td>784.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1289 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Developer   rank\n",
       "EA Canada                                 149    1.0\n",
       "EA Sports                                 142    2.0\n",
       "Capcom                                    126    3.0\n",
       "Ubisoft                                   103    4.0\n",
       "Konami                                     95    5.0\n",
       "...                                       ...    ...\n",
       "Sanzaru Games, Sanzaru Games, Inc.          1  784.0\n",
       "SCEA, Think and Feel                        1  784.0\n",
       "Ubisoft Annecy                              1  784.0\n",
       "Omega Force, Koei Canada                    1  784.0\n",
       "Atomic Games                                1  784.0\n",
       "\n",
       "[1289 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_counts['rank'] = pd.Series(X2.Developer.value_counts().rank(method='min', ascending=False))\n",
    "val_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Score</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Developer_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>28.96</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.45</td>\n",
       "      <td>76.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8</td>\n",
       "      <td>322.0</td>\n",
       "      <td>E</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>Racing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>12.76</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.29</td>\n",
       "      <td>82.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>709.0</td>\n",
       "      <td>E</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>10.93</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8</td>\n",
       "      <td>192.0</td>\n",
       "      <td>E</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DS</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Platform</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>9.14</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2.88</td>\n",
       "      <td>89.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>431.0</td>\n",
       "      <td>E</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>Misc</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>9.18</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.84</td>\n",
       "      <td>58.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>129.0</td>\n",
       "      <td>E</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Platform</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>6.94</td>\n",
       "      <td>4.70</td>\n",
       "      <td>2.24</td>\n",
       "      <td>87.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>594.0</td>\n",
       "      <td>E</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DS</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>Racing</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>7.47</td>\n",
       "      <td>4.13</td>\n",
       "      <td>1.90</td>\n",
       "      <td>91.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>464.0</td>\n",
       "      <td>E</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2007.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>8.03</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.15</td>\n",
       "      <td>80.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>146.0</td>\n",
       "      <td>E</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>X360</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>Misc</td>\n",
       "      <td>Microsoft Game Studios</td>\n",
       "      <td>4.89</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.69</td>\n",
       "      <td>61.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>106.0</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Wii</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>Sports</td>\n",
       "      <td>Nintendo</td>\n",
       "      <td>8.49</td>\n",
       "      <td>2.53</td>\n",
       "      <td>1.77</td>\n",
       "      <td>80.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>52.0</td>\n",
       "      <td>E</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Platform  Year_of_Release     Genre               Publisher  EU_Sales  \\\n",
       "0      Wii           2006.0    Sports                Nintendo     28.96   \n",
       "1      Wii           2008.0    Racing                Nintendo     12.76   \n",
       "2      Wii           2009.0    Sports                Nintendo     10.93   \n",
       "3       DS           2006.0  Platform                Nintendo      9.14   \n",
       "4      Wii           2006.0      Misc                Nintendo      9.18   \n",
       "5      Wii           2009.0  Platform                Nintendo      6.94   \n",
       "6       DS           2005.0    Racing                Nintendo      7.47   \n",
       "7      Wii           2007.0    Sports                Nintendo      8.03   \n",
       "8     X360           2010.0      Misc  Microsoft Game Studios      4.89   \n",
       "9      Wii           2009.0    Sports                Nintendo      8.49   \n",
       "\n",
       "   JP_Sales  Other_Sales  Critic_Score  Critic_Count User_Score  User_Count  \\\n",
       "0      3.77         8.45          76.0          51.0          8       322.0   \n",
       "1      3.79         3.29          82.0          73.0        8.3       709.0   \n",
       "2      3.28         2.95          80.0          73.0          8       192.0   \n",
       "3      6.50         2.88          89.0          65.0        8.5       431.0   \n",
       "4      2.93         2.84          58.0          41.0        6.6       129.0   \n",
       "5      4.70         2.24          87.0          80.0        8.4       594.0   \n",
       "6      4.13         1.90          91.0          64.0        8.6       464.0   \n",
       "7      3.60         2.15          80.0          63.0        7.7       146.0   \n",
       "8      0.24         1.69          61.0          45.0        6.3       106.0   \n",
       "9      2.53         1.77          80.0          33.0        7.4        52.0   \n",
       "\n",
       "  Rating  Developer_freq  \n",
       "0      E              68  \n",
       "1      E              68  \n",
       "2      E              68  \n",
       "3      E              68  \n",
       "4      E              68  \n",
       "5      E              68  \n",
       "6      E              68  \n",
       "7      E              68  \n",
       "8      E               1  \n",
       "9      E              68  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grouping by frequency\n",
    "fq = X2.groupby('Developer').size()\n",
    "# mapping values to X2\n",
    "X2.loc[:, \"{}_freq\".format('Developer')] = X2['Developer'].map(fq)\n",
    "# drop original column\n",
    "X2 = X2.drop(['Developer'], axis = 1)\n",
    "X2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.Developer_freq.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, using the same method as `Developer`, you should convert the following `object` (categorical) features: `Genre`, `Publisher`, `Platform`, and `Rating` using **Frequency Encoding**. We'll encode `User_Score` and `Year_of_Release` later using different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6825, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Score</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Developer_freq</th>\n",
       "      <th>Genre_freq</th>\n",
       "      <th>Publisher_freq</th>\n",
       "      <th>Platform_freq</th>\n",
       "      <th>Rating_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006.0</td>\n",
       "      <td>28.96</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.45</td>\n",
       "      <td>76.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8</td>\n",
       "      <td>322.0</td>\n",
       "      <td>68</td>\n",
       "      <td>943</td>\n",
       "      <td>291</td>\n",
       "      <td>479</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008.0</td>\n",
       "      <td>12.76</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.29</td>\n",
       "      <td>82.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>709.0</td>\n",
       "      <td>68</td>\n",
       "      <td>581</td>\n",
       "      <td>291</td>\n",
       "      <td>479</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009.0</td>\n",
       "      <td>10.93</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8</td>\n",
       "      <td>192.0</td>\n",
       "      <td>68</td>\n",
       "      <td>943</td>\n",
       "      <td>291</td>\n",
       "      <td>479</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006.0</td>\n",
       "      <td>9.14</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2.88</td>\n",
       "      <td>89.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>431.0</td>\n",
       "      <td>68</td>\n",
       "      <td>403</td>\n",
       "      <td>291</td>\n",
       "      <td>464</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006.0</td>\n",
       "      <td>9.18</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.84</td>\n",
       "      <td>58.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>129.0</td>\n",
       "      <td>68</td>\n",
       "      <td>384</td>\n",
       "      <td>291</td>\n",
       "      <td>479</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_of_Release  EU_Sales  JP_Sales  Other_Sales  Critic_Score  \\\n",
       "0           2006.0     28.96      3.77         8.45          76.0   \n",
       "1           2008.0     12.76      3.79         3.29          82.0   \n",
       "2           2009.0     10.93      3.28         2.95          80.0   \n",
       "3           2006.0      9.14      6.50         2.88          89.0   \n",
       "4           2006.0      9.18      2.93         2.84          58.0   \n",
       "\n",
       "   Critic_Count User_Score  User_Count  Developer_freq  Genre_freq  \\\n",
       "0          51.0          8       322.0              68         943   \n",
       "1          73.0        8.3       709.0              68         581   \n",
       "2          73.0          8       192.0              68         943   \n",
       "3          65.0        8.5       431.0              68         403   \n",
       "4          41.0        6.6       129.0              68         384   \n",
       "\n",
       "   Publisher_freq  Platform_freq  Rating_freq  \n",
       "0             291            479         2082  \n",
       "1             291            479         2082  \n",
       "2             291            479         2082  \n",
       "3             291            464         2082  \n",
       "4             291            479         2082  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grouping by frequency\n",
    "genre_fq = X2.groupby('Genre').size()\n",
    "pub_fq = X2.groupby('Publisher').size()\n",
    "platform_fq = X2.groupby('Platform').size()\n",
    "rating_fq = X2.groupby('Rating').size()\n",
    "# mapping values to X2\n",
    "X2.loc[:, \"{}_freq\".format('Genre')] = X2['Genre'].map(genre_fq)\n",
    "X2.loc[:, \"{}_freq\".format('Publisher')] = X2['Publisher'].map(pub_fq)\n",
    "X2.loc[:, \"{}_freq\".format('Platform')] = X2['Platform'].map(platform_fq)\n",
    "X2.loc[:, \"{}_freq\".format('Rating')] = X2['Rating'].map(rating_fq)\n",
    "# drop original column\n",
    "X2 = X2.drop(['Genre'], axis = 1)\n",
    "X2 = X2.drop(['Publisher'], axis = 1)\n",
    "X2 = X2.drop(['Platform'], axis = 1)\n",
    "X2 = X2.drop(['Rating'], axis = 1)\n",
    "\n",
    "\n",
    "print(X2.shape)\n",
    "X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year_of_Release    float64\n",
       "EU_Sales           float64\n",
       "JP_Sales           float64\n",
       "Other_Sales        float64\n",
       "Critic_Score       float64\n",
       "Critic_Count       float64\n",
       "User_Score          object\n",
       "User_Count         float64\n",
       "Developer_freq       int64\n",
       "Genre_freq           int64\n",
       "Publisher_freq       int64\n",
       "Platform_freq        int64\n",
       "Rating_freq          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `User_score` is already numeric, so we just need to convert it to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6825, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Score</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Developer_freq</th>\n",
       "      <th>Genre_freq</th>\n",
       "      <th>Publisher_freq</th>\n",
       "      <th>Platform_freq</th>\n",
       "      <th>Rating_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006.0</td>\n",
       "      <td>28.96</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.45</td>\n",
       "      <td>76.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>68</td>\n",
       "      <td>943</td>\n",
       "      <td>291</td>\n",
       "      <td>479</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008.0</td>\n",
       "      <td>12.76</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.29</td>\n",
       "      <td>82.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>709.0</td>\n",
       "      <td>68</td>\n",
       "      <td>581</td>\n",
       "      <td>291</td>\n",
       "      <td>479</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009.0</td>\n",
       "      <td>10.93</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>68</td>\n",
       "      <td>943</td>\n",
       "      <td>291</td>\n",
       "      <td>479</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006.0</td>\n",
       "      <td>9.14</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2.88</td>\n",
       "      <td>89.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>431.0</td>\n",
       "      <td>68</td>\n",
       "      <td>403</td>\n",
       "      <td>291</td>\n",
       "      <td>464</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006.0</td>\n",
       "      <td>9.18</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.84</td>\n",
       "      <td>58.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>129.0</td>\n",
       "      <td>68</td>\n",
       "      <td>384</td>\n",
       "      <td>291</td>\n",
       "      <td>479</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_of_Release  EU_Sales  JP_Sales  Other_Sales  Critic_Score  \\\n",
       "0           2006.0     28.96      3.77         8.45          76.0   \n",
       "1           2008.0     12.76      3.79         3.29          82.0   \n",
       "2           2009.0     10.93      3.28         2.95          80.0   \n",
       "3           2006.0      9.14      6.50         2.88          89.0   \n",
       "4           2006.0      9.18      2.93         2.84          58.0   \n",
       "\n",
       "   Critic_Count  User_Score  User_Count  Developer_freq  Genre_freq  \\\n",
       "0          51.0         8.0       322.0              68         943   \n",
       "1          73.0         8.3       709.0              68         581   \n",
       "2          73.0         8.0       192.0              68         943   \n",
       "3          65.0         8.5       431.0              68         403   \n",
       "4          41.0         6.6       129.0              68         384   \n",
       "\n",
       "   Publisher_freq  Platform_freq  Rating_freq  \n",
       "0             291            479         2082  \n",
       "1             291            479         2082  \n",
       "2             291            479         2082  \n",
       "3             291            464         2082  \n",
       "4             291            479         2082  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2['User_Score'] = X2['User_Score'].astype(float)\n",
    "print(X2.shape)\n",
    "X2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `Year_of_Release` is an **Ordinal** feature, so we can use **Ordinal Encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2006., 2008., 2009., 2005., 2007., 2010., 2013., 2004., 2002.,\n",
       "       2001., 2011., 2012., 2014., 1997., 1999., 2015., 2016., 2003.,\n",
       "       1998., 1996., 2000., 1994., 1985., 1992., 1988.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.Year_of_Release.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6825, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Score</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Developer_freq</th>\n",
       "      <th>Genre_freq</th>\n",
       "      <th>Publisher_freq</th>\n",
       "      <th>Platform_freq</th>\n",
       "      <th>Rating_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.0</td>\n",
       "      <td>28.96</td>\n",
       "      <td>3.77</td>\n",
       "      <td>8.45</td>\n",
       "      <td>76.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>68</td>\n",
       "      <td>943</td>\n",
       "      <td>291</td>\n",
       "      <td>479</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.0</td>\n",
       "      <td>12.76</td>\n",
       "      <td>3.79</td>\n",
       "      <td>3.29</td>\n",
       "      <td>82.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>709.0</td>\n",
       "      <td>68</td>\n",
       "      <td>581</td>\n",
       "      <td>291</td>\n",
       "      <td>479</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.0</td>\n",
       "      <td>10.93</td>\n",
       "      <td>3.28</td>\n",
       "      <td>2.95</td>\n",
       "      <td>80.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>68</td>\n",
       "      <td>943</td>\n",
       "      <td>291</td>\n",
       "      <td>479</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.0</td>\n",
       "      <td>9.14</td>\n",
       "      <td>6.50</td>\n",
       "      <td>2.88</td>\n",
       "      <td>89.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>431.0</td>\n",
       "      <td>68</td>\n",
       "      <td>403</td>\n",
       "      <td>291</td>\n",
       "      <td>464</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.0</td>\n",
       "      <td>9.18</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.84</td>\n",
       "      <td>58.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.6</td>\n",
       "      <td>129.0</td>\n",
       "      <td>68</td>\n",
       "      <td>384</td>\n",
       "      <td>291</td>\n",
       "      <td>479</td>\n",
       "      <td>2082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_of_Release  EU_Sales  JP_Sales  Other_Sales  Critic_Score  \\\n",
       "0             14.0     28.96      3.77         8.45          76.0   \n",
       "1             16.0     12.76      3.79         3.29          82.0   \n",
       "2             17.0     10.93      3.28         2.95          80.0   \n",
       "3             14.0      9.14      6.50         2.88          89.0   \n",
       "4             14.0      9.18      2.93         2.84          58.0   \n",
       "\n",
       "   Critic_Count  User_Score  User_Count  Developer_freq  Genre_freq  \\\n",
       "0          51.0         8.0       322.0              68         943   \n",
       "1          73.0         8.3       709.0              68         581   \n",
       "2          73.0         8.0       192.0              68         943   \n",
       "3          65.0         8.5       431.0              68         403   \n",
       "4          41.0         6.6       129.0              68         384   \n",
       "\n",
       "   Publisher_freq  Platform_freq  Rating_freq  \n",
       "0             291            479         2082  \n",
       "1             291            479         2082  \n",
       "2             291            479         2082  \n",
       "3             291            464         2082  \n",
       "4             291            479         2082  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure to import the required module at the top in the import cell\n",
    "ord_enc = OrdinalEncoder()\n",
    "X2[\"Year_of_Release\"] = ord_enc.fit_transform(X2[[\"Year_of_Release\"]])\n",
    "\n",
    "print(X2.shape)\n",
    "X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Year_of_Release    float64\n",
       "EU_Sales           float64\n",
       "JP_Sales           float64\n",
       "Other_Sales        float64\n",
       "Critic_Score       float64\n",
       "Critic_Count       float64\n",
       "User_Score         float64\n",
       "User_Count         float64\n",
       "Developer_freq       int64\n",
       "Genre_freq           int64\n",
       "Publisher_freq       int64\n",
       "Platform_freq        int64\n",
       "Rating_freq          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6825, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year_of_Release</th>\n",
       "      <th>EU_Sales</th>\n",
       "      <th>JP_Sales</th>\n",
       "      <th>Other_Sales</th>\n",
       "      <th>Critic_Score</th>\n",
       "      <th>Critic_Count</th>\n",
       "      <th>User_Score</th>\n",
       "      <th>User_Count</th>\n",
       "      <th>Developer_freq</th>\n",
       "      <th>Genre_freq</th>\n",
       "      <th>Publisher_freq</th>\n",
       "      <th>Platform_freq</th>\n",
       "      <th>Rating_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.342432</td>\n",
       "      <td>41.790569</td>\n",
       "      <td>12.886767</td>\n",
       "      <td>31.004904</td>\n",
       "      <td>0.413014</td>\n",
       "      <td>1.147975</td>\n",
       "      <td>0.565560</td>\n",
       "      <td>0.250716</td>\n",
       "      <td>1.061933</td>\n",
       "      <td>0.182752</td>\n",
       "      <td>-0.093788</td>\n",
       "      <td>-0.478813</td>\n",
       "      <td>0.371918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.133507</td>\n",
       "      <td>18.221103</td>\n",
       "      <td>12.956315</td>\n",
       "      <td>11.884655</td>\n",
       "      <td>0.845647</td>\n",
       "      <td>2.292368</td>\n",
       "      <td>0.773902</td>\n",
       "      <td>0.909519</td>\n",
       "      <td>1.061933</td>\n",
       "      <td>-0.556046</td>\n",
       "      <td>-0.093788</td>\n",
       "      <td>-0.478813</td>\n",
       "      <td>0.371918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.371476</td>\n",
       "      <td>15.558627</td>\n",
       "      <td>11.182831</td>\n",
       "      <td>10.624793</td>\n",
       "      <td>0.701436</td>\n",
       "      <td>2.292368</td>\n",
       "      <td>0.565560</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>1.061933</td>\n",
       "      <td>0.182752</td>\n",
       "      <td>-0.093788</td>\n",
       "      <td>-0.478813</td>\n",
       "      <td>0.371918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.342432</td>\n",
       "      <td>12.954346</td>\n",
       "      <td>22.380122</td>\n",
       "      <td>10.365410</td>\n",
       "      <td>1.350385</td>\n",
       "      <td>1.876225</td>\n",
       "      <td>0.912796</td>\n",
       "      <td>0.436270</td>\n",
       "      <td>1.061933</td>\n",
       "      <td>-0.919323</td>\n",
       "      <td>-0.093788</td>\n",
       "      <td>-0.526587</td>\n",
       "      <td>0.371918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.342432</td>\n",
       "      <td>13.012543</td>\n",
       "      <td>9.965734</td>\n",
       "      <td>10.217191</td>\n",
       "      <td>-0.884885</td>\n",
       "      <td>0.627797</td>\n",
       "      <td>-0.406701</td>\n",
       "      <td>-0.077835</td>\n",
       "      <td>1.061933</td>\n",
       "      <td>-0.958100</td>\n",
       "      <td>-0.093788</td>\n",
       "      <td>-0.478813</td>\n",
       "      <td>0.371918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year_of_Release   EU_Sales   JP_Sales  Other_Sales  Critic_Score  \\\n",
       "0        -0.342432  41.790569  12.886767    31.004904      0.413014   \n",
       "1         0.133507  18.221103  12.956315    11.884655      0.845647   \n",
       "2         0.371476  15.558627  11.182831    10.624793      0.701436   \n",
       "3        -0.342432  12.954346  22.380122    10.365410      1.350385   \n",
       "4        -0.342432  13.012543   9.965734    10.217191     -0.884885   \n",
       "\n",
       "   Critic_Count  User_Score  User_Count  Developer_freq  Genre_freq  \\\n",
       "0      1.147975    0.565560    0.250716        1.061933    0.182752   \n",
       "1      2.292368    0.773902    0.909519        1.061933   -0.556046   \n",
       "2      2.292368    0.565560    0.029412        1.061933    0.182752   \n",
       "3      1.876225    0.912796    0.436270        1.061933   -0.919323   \n",
       "4      0.627797   -0.406701   -0.077835        1.061933   -0.958100   \n",
       "\n",
       "   Publisher_freq  Platform_freq  Rating_freq  \n",
       "0       -0.093788      -0.478813     0.371918  \n",
       "1       -0.093788      -0.478813     0.371918  \n",
       "2       -0.093788      -0.478813     0.371918  \n",
       "3       -0.093788      -0.526587     0.371918  \n",
       "4       -0.093788      -0.478813     0.371918  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize X2 using X2.mean() and X2.std()  NOTE: The output is provided for your reference.\n",
    "X2 = (X2-X2.mean())/X2.std()\n",
    "print(X2.shape)\n",
    "X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5460, 13)\n",
      "(5460,)\n",
      "(1365, 13)\n",
      "(1365,)\n"
     ]
    }
   ],
   "source": [
    "# Split the data to train and test with ratio of 80/20 for train/test respectively\n",
    "# NOTE: Make sure to split X2 and y2, NOT X and y!\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2,y2,test_size=0.20)\n",
    "print(X2_train.shape)\n",
    "print(y2_train.shape)\n",
    "print(X2_test.shape)\n",
    "print(y2_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building NN for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build a sequential NN model `nn_reg` with proper configurations for the regression task.\n",
    "\n",
    "> **Hints**:\n",
    "> - `input_dim` of the first layer should match with the number of features in `X2`\n",
    "\n",
    "> - ReLU is usually a good activation function for the hidden layers, but you may also try other activation functions and/or initialization strategies.\n",
    "\n",
    "> - Note that the activation function and number of neurons in the output layer are determined by the type of ML task i.e. Regression. Also note that there is no limit/restriction defined on the predicted value `NA_Sales`. As mentioned in the lectures, you should NOT modify/amend the problem defintion and your neural network should precisely match with the ML task specifications, which in this case is **a regression task with an unbounded output**.\n",
    "\n",
    "> - The common loss functions for regression are `mae` and `mse` and the metric is usually the same as loss for regression. To keep the results consistent with the requirements, you should use `mae` as the loss function, and the metric would also be `mae`.\n",
    "\n",
    "> - You're going to use callback and early stopping to determine the optimal number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font color='red'>**Test MAE Loss Requirement**</font>: Your `nn_reg` model's `mae` loss on `X2_test` should not exceed **0.20**, i.e. the test `mae` loss should be 0.20 or lower. Otherwise, your `nn_reg` will get zero points for this part, so you must fine-tune your `nn_reg` and `compile` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Build nn_reg with appropriate layers for regression\n",
    "\n",
    "    Hint1: input_dim of the first layer should match with the number of features in X2\n",
    "    \n",
    "    Hint2: The activation function and number of neurons in the output layer are determined\n",
    "    by the type of ML task i.e. Regression\n",
    "    \n",
    "    Hint3: If you observed overfitting in the history plot,\n",
    "    consider using regularization techniques such as Dropout, and/or Batch Normalization '''\n",
    "\n",
    "nn_reg = keras.models.Sequential([\n",
    "    keras.layers.Dense(13, activation=\"relu\", input_shape=[13]),\n",
    "     tf.keras.layers.Dropout(0.1),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer of your choice and set its learning_rate (you may need to tune it)\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next, you're going to create `callbacks` and `EarlyStopping`.\n",
    "\n",
    "> **Note:** The `patience` parameter is the number of epochs to monitor for improvement in `EarlyStopping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create a callback with EarlyStopping and,\n",
    "    monitor='val_loss',  patience=10 and restore_best_weights=True '''\n",
    "early_stopping_reg = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Compile nn_reg with loss='mae', optimizer=optimizer and metrics=['mae'] '''\n",
    "nn_reg.compile(loss=\"mae\", metrics=['mae'], optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now run the training, sit back and let tf/keras determine the optimal number of epochs! Although the `EPOCHS` is set to 200, the training would usually stop sooner with `EarlyStopping`. You may change/increase `EPOCHS` and/or any other `compile` or `nn_reg` hyperparameter that is necessary to achieve the required test `mae` loss, i.e. 0.2 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.4653 - mae: 0.4653 - val_loss: 0.3364 - val_mae: 0.3364\n",
      "Epoch 2/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.3442 - mae: 0.3442 - val_loss: 0.2887 - val_mae: 0.2887\n",
      "Epoch 3/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2967 - mae: 0.2967 - val_loss: 0.2581 - val_mae: 0.2581\n",
      "Epoch 4/200\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.2722 - mae: 0.2722 - val_loss: 0.2439 - val_mae: 0.2439\n",
      "Epoch 5/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2578 - mae: 0.2578 - val_loss: 0.2373 - val_mae: 0.2373\n",
      "Epoch 6/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2450 - mae: 0.2450 - val_loss: 0.2336 - val_mae: 0.2336\n",
      "Epoch 7/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2327 - mae: 0.2327 - val_loss: 0.2309 - val_mae: 0.2309\n",
      "Epoch 8/200\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.2287 - mae: 0.2287 - val_loss: 0.2291 - val_mae: 0.2291\n",
      "Epoch 9/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2287 - mae: 0.2287 - val_loss: 0.2278 - val_mae: 0.2278\n",
      "Epoch 10/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2238 - mae: 0.2238 - val_loss: 0.2234 - val_mae: 0.2234\n",
      "Epoch 11/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2148 - mae: 0.2148 - val_loss: 0.2217 - val_mae: 0.2217\n",
      "Epoch 12/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2191 - mae: 0.2191 - val_loss: 0.2216 - val_mae: 0.2216\n",
      "Epoch 13/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2170 - mae: 0.2170 - val_loss: 0.2197 - val_mae: 0.2197\n",
      "Epoch 14/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2153 - mae: 0.2153 - val_loss: 0.2201 - val_mae: 0.2201\n",
      "Epoch 15/200\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.2107 - mae: 0.2107 - val_loss: 0.2158 - val_mae: 0.2158\n",
      "Epoch 16/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2137 - mae: 0.2137 - val_loss: 0.2150 - val_mae: 0.2150\n",
      "Epoch 17/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2113 - mae: 0.2113 - val_loss: 0.2152 - val_mae: 0.2152\n",
      "Epoch 18/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2123 - mae: 0.2123 - val_loss: 0.2121 - val_mae: 0.2121\n",
      "Epoch 19/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.2067 - mae: 0.2067 - val_loss: 0.2111 - val_mae: 0.2111\n",
      "Epoch 20/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.2085 - mae: 0.2085 - val_loss: 0.2096 - val_mae: 0.2096\n",
      "Epoch 21/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.2050 - mae: 0.2050 - val_loss: 0.2102 - val_mae: 0.2102\n",
      "Epoch 22/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.2030 - mae: 0.2030 - val_loss: 0.2090 - val_mae: 0.2090\n",
      "Epoch 23/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.2045 - mae: 0.2045 - val_loss: 0.2075 - val_mae: 0.2075\n",
      "Epoch 24/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.2064 - mae: 0.2064 - val_loss: 0.2088 - val_mae: 0.2088\n",
      "Epoch 25/200\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.2030 - mae: 0.2030 - val_loss: 0.2081 - val_mae: 0.2081\n",
      "Epoch 26/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2031 - mae: 0.2031 - val_loss: 0.2073 - val_mae: 0.2073\n",
      "Epoch 27/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2028 - mae: 0.2028 - val_loss: 0.2067 - val_mae: 0.2067\n",
      "Epoch 28/200\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.2010 - mae: 0.2010 - val_loss: 0.2070 - val_mae: 0.2070\n",
      "Epoch 29/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2004 - mae: 0.2004 - val_loss: 0.2070 - val_mae: 0.2070\n",
      "Epoch 30/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2010 - mae: 0.2010 - val_loss: 0.2059 - val_mae: 0.2059\n",
      "Epoch 31/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.1983 - mae: 0.1983 - val_loss: 0.2056 - val_mae: 0.2056\n",
      "Epoch 32/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2007 - mae: 0.2007 - val_loss: 0.2058 - val_mae: 0.2058\n",
      "Epoch 33/200\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.1986 - mae: 0.1986 - val_loss: 0.2054 - val_mae: 0.2054\n",
      "Epoch 34/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.2005 - mae: 0.2005 - val_loss: 0.2065 - val_mae: 0.2065\n",
      "Epoch 35/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.1994 - mae: 0.1994 - val_loss: 0.2050 - val_mae: 0.2050\n",
      "Epoch 36/200\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1970 - mae: 0.1970 - val_loss: 0.2053 - val_mae: 0.2053\n",
      "Epoch 37/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1999 - mae: 0.1999 - val_loss: 0.2052 - val_mae: 0.2052\n",
      "Epoch 38/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1988 - mae: 0.1988 - val_loss: 0.2048 - val_mae: 0.2048\n",
      "Epoch 39/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1967 - mae: 0.1967 - val_loss: 0.2040 - val_mae: 0.2040\n",
      "Epoch 40/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1966 - mae: 0.1966 - val_loss: 0.2039 - val_mae: 0.2039\n",
      "Epoch 41/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1989 - mae: 0.1989 - val_loss: 0.2049 - val_mae: 0.2049\n",
      "Epoch 42/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.2001 - mae: 0.2001 - val_loss: 0.2039 - val_mae: 0.2039\n",
      "Epoch 43/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1988 - mae: 0.1988 - val_loss: 0.2039 - val_mae: 0.2039\n",
      "Epoch 44/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1984 - mae: 0.1984 - val_loss: 0.2045 - val_mae: 0.2045\n",
      "Epoch 45/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1986 - mae: 0.1986 - val_loss: 0.2031 - val_mae: 0.2031\n",
      "Epoch 46/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1967 - mae: 0.1967 - val_loss: 0.2041 - val_mae: 0.2041\n",
      "Epoch 47/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1962 - mae: 0.1962 - val_loss: 0.2037 - val_mae: 0.2037\n",
      "Epoch 48/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.2023 - mae: 0.2023 - val_loss: 0.2029 - val_mae: 0.2029\n",
      "Epoch 49/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1982 - mae: 0.1982 - val_loss: 0.2019 - val_mae: 0.2019\n",
      "Epoch 50/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1976 - mae: 0.1976 - val_loss: 0.2022 - val_mae: 0.2022\n",
      "Epoch 51/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.1998 - mae: 0.1998 - val_loss: 0.2023 - val_mae: 0.2023\n",
      "Epoch 52/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.1977 - mae: 0.1977 - val_loss: 0.2026 - val_mae: 0.2026\n",
      "Epoch 53/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.1938 - mae: 0.1938 - val_loss: 0.2022 - val_mae: 0.2022\n",
      "Epoch 54/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.1982 - mae: 0.1982 - val_loss: 0.2020 - val_mae: 0.2020\n",
      "Epoch 55/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.1947 - mae: 0.1947 - val_loss: 0.2025 - val_mae: 0.2025\n",
      "Epoch 56/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.1931 - mae: 0.1931 - val_loss: 0.2023 - val_mae: 0.2023\n",
      "Epoch 57/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.1971 - mae: 0.1971 - val_loss: 0.2016 - val_mae: 0.2016\n",
      "Epoch 58/200\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.1941 - mae: 0.1941 - val_loss: 0.2005 - val_mae: 0.2005\n",
      "Epoch 59/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.1962 - mae: 0.1962 - val_loss: 0.2012 - val_mae: 0.2012\n",
      "Epoch 60/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1909 - mae: 0.1909 - val_loss: 0.2002 - val_mae: 0.2002\n",
      "Epoch 61/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1931 - mae: 0.1931 - val_loss: 0.2003 - val_mae: 0.2003\n",
      "Epoch 62/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1945 - mae: 0.1945 - val_loss: 0.1989 - val_mae: 0.1989\n",
      "Epoch 63/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1934 - mae: 0.1934 - val_loss: 0.2005 - val_mae: 0.2005\n",
      "Epoch 64/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1934 - mae: 0.1934 - val_loss: 0.1989 - val_mae: 0.1989\n",
      "Epoch 65/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1934 - mae: 0.1934 - val_loss: 0.1973 - val_mae: 0.1973\n",
      "Epoch 66/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1907 - mae: 0.1907 - val_loss: 0.1989 - val_mae: 0.1989\n",
      "Epoch 67/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1917 - mae: 0.1917 - val_loss: 0.1983 - val_mae: 0.1983\n",
      "Epoch 68/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1903 - mae: 0.1903 - val_loss: 0.1981 - val_mae: 0.1981\n",
      "Epoch 69/200\n",
      "154/154 [==============================] - 1s 4ms/step - loss: 0.1895 - mae: 0.1895 - val_loss: 0.1994 - val_mae: 0.1994\n",
      "Epoch 70/200\n",
      "154/154 [==============================] - 1s 6ms/step - loss: 0.1912 - mae: 0.1912 - val_loss: 0.1964 - val_mae: 0.1964\n",
      "Epoch 71/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.1882 - mae: 0.1882 - val_loss: 0.1963 - val_mae: 0.1963\n",
      "Epoch 72/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.1887 - mae: 0.1887 - val_loss: 0.1967 - val_mae: 0.1967\n",
      "Epoch 73/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.1912 - mae: 0.1912 - val_loss: 0.1949 - val_mae: 0.1949\n",
      "Epoch 74/200\n",
      "154/154 [==============================] - 1s 5ms/step - loss: 0.1919 - mae: 0.1919 - val_loss: 0.1965 - val_mae: 0.1965\n",
      "Epoch 75/200\n",
      "154/154 [==============================] - 0s 3ms/step - loss: 0.1892 - mae: 0.1892 - val_loss: 0.1944 - val_mae: 0.1944\n",
      "Epoch 76/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1871 - mae: 0.1871 - val_loss: 0.1973 - val_mae: 0.1973\n",
      "Epoch 77/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1910 - mae: 0.1910 - val_loss: 0.1952 - val_mae: 0.1952\n",
      "Epoch 78/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1875 - mae: 0.1875 - val_loss: 0.1934 - val_mae: 0.1934\n",
      "Epoch 79/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1909 - mae: 0.1909 - val_loss: 0.1955 - val_mae: 0.1955\n",
      "Epoch 80/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1872 - mae: 0.1872 - val_loss: 0.1938 - val_mae: 0.1938\n",
      "Epoch 81/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1948 - mae: 0.1948 - val_loss: 0.1926 - val_mae: 0.1926\n",
      "Epoch 82/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1843 - mae: 0.1843 - val_loss: 0.1927 - val_mae: 0.1927\n",
      "Epoch 83/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1852 - mae: 0.1852 - val_loss: 0.1942 - val_mae: 0.1942\n",
      "Epoch 84/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1862 - mae: 0.1862 - val_loss: 0.1914 - val_mae: 0.1914\n",
      "Epoch 85/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1880 - mae: 0.1880 - val_loss: 0.1928 - val_mae: 0.1928\n",
      "Epoch 86/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1870 - mae: 0.1870 - val_loss: 0.1922 - val_mae: 0.1922\n",
      "Epoch 87/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1849 - mae: 0.1849 - val_loss: 0.1915 - val_mae: 0.1915\n",
      "Epoch 88/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1852 - mae: 0.1852 - val_loss: 0.1929 - val_mae: 0.1929\n",
      "Epoch 89/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1894 - mae: 0.1894 - val_loss: 0.1909 - val_mae: 0.1909\n",
      "Epoch 90/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1861 - mae: 0.1861 - val_loss: 0.1915 - val_mae: 0.1915\n",
      "Epoch 91/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1881 - mae: 0.1881 - val_loss: 0.1913 - val_mae: 0.1913\n",
      "Epoch 92/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1828 - mae: 0.1828 - val_loss: 0.1914 - val_mae: 0.1914\n",
      "Epoch 93/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1828 - mae: 0.1828 - val_loss: 0.1908 - val_mae: 0.1908\n",
      "Epoch 94/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1889 - mae: 0.1889 - val_loss: 0.1903 - val_mae: 0.1903\n",
      "Epoch 95/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1850 - mae: 0.1850 - val_loss: 0.1901 - val_mae: 0.1901\n",
      "Epoch 96/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1879 - mae: 0.1879 - val_loss: 0.1914 - val_mae: 0.1914\n",
      "Epoch 97/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1845 - mae: 0.1845 - val_loss: 0.1895 - val_mae: 0.1895\n",
      "Epoch 98/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1868 - mae: 0.1868 - val_loss: 0.1926 - val_mae: 0.1926\n",
      "Epoch 99/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1895 - mae: 0.1895 - val_loss: 0.1902 - val_mae: 0.1902\n",
      "Epoch 100/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1883 - mae: 0.1883 - val_loss: 0.1902 - val_mae: 0.1902\n",
      "Epoch 101/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1858 - mae: 0.1858 - val_loss: 0.1902 - val_mae: 0.1902\n",
      "Epoch 102/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1832 - mae: 0.1832 - val_loss: 0.1904 - val_mae: 0.1904\n",
      "Epoch 103/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1853 - mae: 0.1853 - val_loss: 0.1883 - val_mae: 0.1883\n",
      "Epoch 104/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1814 - mae: 0.1814 - val_loss: 0.1899 - val_mae: 0.1899\n",
      "Epoch 105/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1837 - mae: 0.1837 - val_loss: 0.1884 - val_mae: 0.1884\n",
      "Epoch 106/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1875 - mae: 0.1875 - val_loss: 0.1891 - val_mae: 0.1891\n",
      "Epoch 107/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1867 - mae: 0.1867 - val_loss: 0.1896 - val_mae: 0.1896\n",
      "Epoch 108/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1840 - mae: 0.1840 - val_loss: 0.1893 - val_mae: 0.1893\n",
      "Epoch 109/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1830 - mae: 0.1830 - val_loss: 0.1887 - val_mae: 0.1887\n",
      "Epoch 110/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1805 - mae: 0.1805 - val_loss: 0.1902 - val_mae: 0.1902\n",
      "Epoch 111/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1836 - mae: 0.1836 - val_loss: 0.1887 - val_mae: 0.1887\n",
      "Epoch 112/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1850 - mae: 0.1850 - val_loss: 0.1908 - val_mae: 0.1908\n",
      "Epoch 113/200\n",
      "154/154 [==============================] - 0s 2ms/step - loss: 0.1855 - mae: 0.1855 - val_loss: 0.1891 - val_mae: 0.1891\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "''' Fit nn_reg on X2_train, y2_train, and with epochs=EPOCHS, callbacks=[early_stopping_reg], and\n",
    "    validation_split=0.1 '''\n",
    "nn_reg_history = nn_reg.fit(X2_train, y2_train, epochs=EPOCHS, validation_split=0.1, callbacks=[early_stopping_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' plot the history with a figsize of (10,5)\n",
    "    the plot style should be reset back to 'default'\n",
    "    the plot should display the grid and the whole range of values for loss and mae '''\n",
    "pd.DataFrame(nn_reg_history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/43 - 0s - loss: 0.1655 - mae: 0.1655 - 163ms/epoch - 4ms/step\n"
     ]
    }
   ],
   "source": [
    "''' Maximum reg_loss: 0.20 \n",
    "    if reg_loss is higher than 0.2, you should rebuild your nn_reg with new hyperparameters and retrain it '''\n",
    "\n",
    "# Evaluate nn_reg on X2_test, y2_test\n",
    "reg_loss, reg_mae = nn_reg.evaluate(X2_test, y2_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips To Overcome Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the provided `nn_reg_history` plot, it seems there is no severe overfitting because `mae` and `val_mae` are decreasing almost at the same rate. That's because regularization techniques have been used in building `nn_reg`, so you should probably use techniques like batch norm and dropout too, although the beauty of the NNs is that they are so flexible and there might be multiple solutions/architectures that work for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NNs are so prone to overfitting because they have lots of trainable parameters and hence a large degree of freedom which makes them have a high variance.\n",
    "\n",
    "> As discussed in the lectures, there are various regularization techniques to tackle overfitting in NNs and DNNs. You've already used one regularization technique so far in training `nn_reg` and that was `EarlyStopping`. However, if you notice in your history plot that your model has run into overfitting, consider using other techniques, such as `l2`, `Dropout` and `BatchNormalization`. Recall that although `BatchNormalization` was intended to help with unstable gradients problem, it has regularization effects too.\n",
    "\n",
    "> If you want to use both `Dropout` and `BatchNormalization`, it's easier to use them in the order as in the following cell, i.e. first the dense layer with activation function followed by batch norm followed by dropout, although as discussed in the lectures, BN can be applied before the activation function too. You should certainly fine-tune the probability of `Dropout` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    " ''' Do NOT write anything in this cell!\n",
    "     This is just to show you how to use BN and dropout together'''\n",
    "    ...\n",
    "    tf.keras.layers.Dense(10, activation = \"relu\"), # this is just an example, your dense layers might differ\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2), # dropout probability = 0.2 which may be tuned\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-II - Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANSWER THE FOLLOWING QUESTIONS HERE:\n",
    "\n",
    "**Q3 [5 points]** <br>\n",
    "(**a - 3 points**) - Using eraly stopping and callbacks, how may epochs did the training run for `nn_reg`? Find the attribute in `nn_reg_history` object that logs the number of epochs. Write the code to get number of epochs in the following cell.\n",
    "\n",
    "Answer: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    }
   ],
   "source": [
    "''' Get the number of epochs from nn_reg_history object\n",
    "    Hint: You should use an attribute of the `history` object that gets the list of all epochs,\n",
    "    and report its length; number of epochs may vary, dependeing on when EarlyStopping stopped training\n",
    "    '''\n",
    "print(np.count_nonzero(nn_reg_history.history['loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(**b - 2 points**) - How did the training of `nn_reg` stop? Your answer should exactly mention the criteria for when the training stops given the specifications. EXPLAIN CLEARLY AND COMPLETELY IN A FEW SENTENCES.\n",
    "\n",
    "Answer: \n",
    "\n",
    "We used earlystopping with the parameters monitor='val_loss', patience=10, restore_best_weights=True. This means that early stopping will stop the training if it observes that the value of val_loss has not improved for the past 10 epochs. Then it resotes the best weights from the model with the best results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4 [5 points]** - On the history plot of `nn_reg`, you have four colors displayed in the legend, but you can see only two of them (only the orange and red curves are displayed). Explain why?\n",
    "\n",
    "Answer: \n",
    "\n",
    "We use MAE for both our cost function and our evauation metric. This means that durring training they both calculated the same values and therefore the lines for val_loss and val_mae is exactly overlapped with loss and mae, making not visable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading\n",
    "\n",
    "Assignment-4 has a maximum of 100 points. Make sure that you get the correct outputs/plots for all cells that you implement and give complete answers to all questions. Also, your notebook should be written with no grammatical and spelling errors and should be easy-to-read.\n",
    "\n",
    "The breakdown of the 100 points is as follows:\n",
    "\n",
    "- Part-I Multi-Class Classification: [total 50 points]\n",
    "    - Implementation of `baseline_model`: 10 points - **val_accuracy Requirement**: 0.85 after the last epoch otherwise zero points\n",
    "    - Implementation of `nn_clf`: 10 points - **Test Accuracy Requirement**: accuracy on `X_test` should be 0.99 otherwise zero points\n",
    "    - Accuracy vs Learning Rate plot: 20 points - Incomplete/wrong plots get zero points\n",
    "    - Questions: 10 points (5 points each)\n",
    "\n",
    "\n",
    "- Part-II Regression on `NA_Sales`: [total 50 points]\n",
    "    - Implementation of nn_reg: 40 points - **Test MAE Loss Requirement**: 0.20 for `mae` loss on `X2_test` otherwise zero points \n",
    "    - Questions: 10 points (5 points each)\n",
    "   \n",
    "\n",
    "<b>Note: </b>Follow the instructions of each section carefully. Up to 10 points may be deducted if your submitted notebook is not easy to read and follow or if it has grammatical, spelling or formatting issues.\n",
    "\n",
    "Grading will be based on \n",
    "\n",
    "  * correct implementation and results\n",
    "  * correct answer to the questions\n",
    "  * complete running of all required cells\n",
    "  * readability of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name your notebook ```Lastname-A4.ipynb```. Submit the completed notebook using the ```Assignment-4``` link on Blackboard.\n",
    "  \n",
    "<font color=red><b>Due Date: Thursday April 28th, 11:59PM.</b></font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
